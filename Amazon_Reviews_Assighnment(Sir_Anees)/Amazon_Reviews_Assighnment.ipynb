{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Amazon_Reviews_Assighnment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29o-oFpYGAZ1",
        "colab_type": "text"
      },
      "source": [
        "Download Following Dataset and use five text(use randomly generated 5 numbers and extract the data of the generated numbers from the dataset) to test the assignments. https://www.kaggle.com/bittlingmayer/amazonreviews\n",
        "\n",
        "1) one-hot coding of the following data using book codes (naive codes) of listing 6.1.\n",
        "\n",
        "2) Also, provide one-hot coding using Keras built-in function (listing 6.3).\n",
        "\n",
        "3) provide one-hot coding with hashing (listing 6.4)\n",
        "\n",
        "4) Validate the how much they are similar. Try to maximize the similarity (ideally should be 100% the same). Specify the reason if both are not the same.\n",
        "\n",
        "5) Try to implement word-embedding using code given in listing (6.7) and shared with me the embedding array as well as the word dictionary.\n",
        "\n",
        "6) From 6.8 code pre-trained word-embeddings\n",
        "\n",
        "7) Apply RNN to the given text (listing 6.21) and provide output\n",
        "\n",
        "8) Match the results of RNN (step 7) with the step 5 and also with step\n",
        "\n",
        "Share the output of the analysis.\n",
        "Submit your assignment here https://forms.gle/ta88MoyAC8Fmsd4n7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYHwknv5GN7w",
        "colab_type": "code",
        "outputId": "ad75d390-cd02-42c2-eb6f-350978afda67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# mounting Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHNIWUYV9Ntq",
        "colab_type": "code",
        "outputId": "8dfe294e-4e5d-4bd0-b81a-c1fa86127694",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# Datasets in my Google drive\n",
        "import os\n",
        "os.listdir(\"/content/drive/My Drive/ML and AI/Datasets\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cat vs dog',\n",
              " 'compresive_strength_concrete',\n",
              " 'Mnist_fashion_dataset',\n",
              " 'Boston Housing dataset',\n",
              " 'sonar dataset',\n",
              " 'Iris dataset',\n",
              " 'diabetes dataset',\n",
              " 'bbc dataset',\n",
              " 'IMDB dataset',\n",
              " 'jena_climate',\n",
              " 'Amazon_Reviews']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4l_9MlRS9aPD",
        "colab_type": "code",
        "outputId": "bcbf539a-1f24-484f-8bc1-89380afa0305",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Current location\n",
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qDjusNe9mJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# unzipped in current location\n",
        "!unzip \"/content/drive/My Drive/ML and AI/Datasets/Amazon_Reviews/Amazon Reviews for Sentiment Analysis.zip\"\n",
        "# then move the both extracted file to Datasets folder\n",
        "import shutil\n",
        "shutil.move(\"train.ft.txt\", \"/content/drive/My Drive/ML and AI/Datasets/Amazon_Reviews\")\n",
        "shutil.move(\"test.ft.txt\", \"/content/drive/My Drive/ML and AI/Datasets/Amazon_Reviews\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QX0QnY1D94ab",
        "colab_type": "code",
        "outputId": "81088346-a36a-45d0-ddc7-3f7075e9ad5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "os.listdir(\"/content/drive/My Drive/ML and AI/Datasets/Amazon_Reviews\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['test.ft.txt', 'train.ft.txt', 'Amazon Reviews for Sentiment Analysis.zip']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PhUTugC985g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the training data\n",
        "data = open(\"/content/drive/My Drive/ML and AI/Datasets/Amazon_Reviews/train.ft.txt\", \"r\")\n",
        "data = data.read()\n",
        "data = data.splitlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekhCNaTe-Lz1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating labels\n",
        "labels = []\n",
        "x = 0\n",
        "y = 1\n",
        "\n",
        "for j in range(len(data)):\n",
        "  for i in data[x:y]:\n",
        "    if i[:10] == \"__label__2\":\n",
        "      labels.append(0)\n",
        "    elif i[:10] == \"__label__1\":\n",
        "      labels.append(1)\n",
        "    else:\n",
        "      print(\"fault\")\n",
        "    x+=1\n",
        "    y+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns-cgFPs-Ybv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cleaning text\n",
        "texts = []\n",
        "x = 0\n",
        "y = 1\n",
        "\n",
        "for j in range(len(data)):\n",
        "  for i in data[x:y]:\n",
        "      texts.append(i[11:])\n",
        "  x+=1\n",
        "  y+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-dxkDIx-bU2",
        "colab_type": "code",
        "outputId": "29e0b242-9bec-451d-a7aa-f0c9e18b5947",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "# saving in dataframe\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(list(zip(labels, texts)), \n",
        "               columns =['labels', 'reviews']) \n",
        "df.head(12)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>labels</th>\n",
              "      <th>reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>an absolute masterpiece: I am quite sure any o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>Buyer beware: This is a self-published book, a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>Glorious story: I loved Whisper of the wicked ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>A FIVE STAR BOOK: I just finished reading Whis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>Whispers of the Wicked Saints: This was a easy...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1</td>\n",
              "      <td>The Worst!: A complete waste of time. Typograp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>Great book: This was a great book,I just could...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    labels                                            reviews\n",
              "0        0  Stuning even for the non-gamer: This sound tra...\n",
              "1        0  The best soundtrack ever to anything.: I'm rea...\n",
              "2        0  Amazing!: This soundtrack is my favorite music...\n",
              "3        0  Excellent Soundtrack: I truly like this soundt...\n",
              "4        0  Remember, Pull Your Jaw Off The Floor After He...\n",
              "5        0  an absolute masterpiece: I am quite sure any o...\n",
              "6        1  Buyer beware: This is a self-published book, a...\n",
              "7        0  Glorious story: I loved Whisper of the wicked ...\n",
              "8        0  A FIVE STAR BOOK: I just finished reading Whis...\n",
              "9        0  Whispers of the Wicked Saints: This was a easy...\n",
              "10       1  The Worst!: A complete waste of time. Typograp...\n",
              "11       0  Great book: This was a great book,I just could..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pnu_NrnEv4C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##########################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsAP8hSHO5fK",
        "colab_type": "text"
      },
      "source": [
        "data set is really large and taking alot of computational power and time.\n",
        "\n",
        "We can use it in chunks but it will complecate our codes.\n",
        "\n",
        "So for this Assighnment we will use a part of tha data not all of it.\n",
        "\n",
        "You can undo this by not running cell bellow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfoFBA83Ou2Z",
        "colab_type": "code",
        "outputId": "a691a842-7c13-44c8-c906-ecf9ec76cb45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "Slice = 360000\n",
        "print(\"Total length before :\",len(df)) # we are going to use first 360000\n",
        "df = df.loc[1:Slice]\n",
        "print(\"Total length Now :\",len(df))\n",
        "# also going to save slice data in list\n",
        "texts = df[\"reviews\"].to_numpy()\n",
        "labels = df[\"labels\"].to_numpy()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total length before : 3600000\n",
            "Total length Now : 360000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5sOHg0NOu8S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###########################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4vDNNIM4u0j",
        "colab_type": "text"
      },
      "source": [
        "1) one-hot coding of the following data using book codes (naive codes) of listing 6.1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cam71i8-5c_f",
        "colab_type": "code",
        "outputId": "4a01f3d6-363c-48e3-d58d-70e5903ae120",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"text total length :\",len(texts)) # texts type List\n",
        "samples = texts[:36] # using only first 36 reviews"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text total length : 360000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A73Aw4HT4R2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "#samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "token_index = {}\n",
        "\n",
        "for sample in samples:\n",
        "  for word in sample.split():\n",
        "    if word not in token_index:\n",
        "      token_index[word] = len(token_index) + 1\n",
        "\n",
        "max_length = 10\n",
        "results = np.zeros(shape=(len(samples),\n",
        "max_length,\n",
        "max(token_index.values()) + 1))\n",
        "\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "    index = token_index.get(word)\n",
        "results[i, j, index] = 1."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ah2glN3h5Kem",
        "colab_type": "code",
        "outputId": "6fcde6b1-edd6-449d-aaf9-896583821e20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(token_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'The': 1, 'best': 2, 'soundtrack': 3, 'ever': 4, 'to': 5, 'anything.:': 6, \"I'm\": 7, 'reading': 8, 'a': 9, 'lot': 10, 'of': 11, 'reviews': 12, 'saying': 13, 'that': 14, 'this': 15, 'is': 16, 'the': 17, \"'game\": 18, \"soundtrack'\": 19, 'and': 20, 'I': 21, 'figured': 22, \"I'd\": 23, 'write': 24, 'review': 25, 'disagree': 26, 'bit.': 27, 'This': 28, 'in': 29, 'my': 30, 'opinino': 31, 'Yasunori': 32, \"Mitsuda's\": 33, 'ultimate': 34, 'masterpiece.': 35, 'music': 36, 'timeless': 37, 'been': 38, 'listening': 39, 'it': 40, 'for': 41, 'years': 42, 'now': 43, 'its': 44, 'beauty': 45, 'simply': 46, 'refuses': 47, 'fade.The': 48, 'price': 49, 'tag': 50, 'on': 51, 'pretty': 52, 'staggering': 53, 'must': 54, 'say,': 55, 'but': 56, 'if': 57, 'you': 58, 'are': 59, 'going': 60, 'buy': 61, 'any': 62, 'cd': 63, 'much': 64, 'money,': 65, 'only': 66, 'one': 67, 'feel': 68, 'would': 69, 'be': 70, 'worth': 71, 'every': 72, 'penny.': 73, 'Amazing!:': 74, 'favorite': 75, 'all': 76, 'time,': 77, 'hands': 78, 'down.': 79, 'intense': 80, 'sadness': 81, '\"Prisoners': 82, 'Fate\"': 83, '(which': 84, 'means': 85, 'more': 86, \"you've\": 87, 'played': 88, 'game)': 89, 'hope': 90, '\"A': 91, 'Distant': 92, 'Promise\"': 93, '\"Girl': 94, 'who': 95, 'Stole': 96, 'Star\"': 97, 'have': 98, 'an': 99, 'important': 100, 'inspiration': 101, 'me': 102, 'personally': 103, 'throughout': 104, 'teen': 105, 'years.': 106, 'higher': 107, 'energy': 108, 'tracks': 109, 'like': 110, '\"Chrono': 111, 'Cross': 112, '~': 113, \"Time's\": 114, 'Scar~\",': 115, '\"Time': 116, 'Dreamwatch\",': 117, '\"Chronomantique\"': 118, '(indefinably': 119, 'remeniscent': 120, 'Chrono': 121, 'Trigger)': 122, 'absolutely': 123, 'superb': 124, 'as': 125, 'well.This': 126, 'amazing': 127, 'music,': 128, 'probably': 129, \"composer's\": 130, 'work': 131, '(I': 132, \"haven't\": 133, 'heard': 134, 'Xenogears': 135, 'soundtrack,': 136, 'so': 137, \"can't\": 138, 'say': 139, 'sure),': 140, 'even': 141, 'never': 142, 'game,': 143, 'twice': 144, 'it.I': 145, 'wish': 146, 'could': 147, 'give': 148, '6': 149, 'stars.': 150, 'Excellent': 151, 'Soundtrack:': 152, 'truly': 153, 'enjoy': 154, 'video': 155, 'game': 156, 'music.': 157, 'most': 158, 'here': 159, \"it's\": 160, 'relaxing': 161, 'peaceful.On': 162, 'disk': 163, 'one.': 164, 'favorites': 165, 'Scars': 166, 'Of': 167, 'Time,': 168, 'Between': 169, 'Life': 170, 'Death,': 171, 'Forest': 172, 'Illusion,': 173, 'Fortress': 174, 'Ancient': 175, 'Dragons,': 176, 'Lost': 177, 'Fragment,': 178, 'Drowned': 179, 'Valley.Disk': 180, 'Two:': 181, 'Draggons,': 182, 'Galdorb': 183, '-': 184, 'Home,': 185, 'Chronomantique,': 186, 'Prisoners': 187, 'Fate,': 188, 'Gale,': 189, 'girlfriend': 190, 'likes': 191, 'ZelbessDisk': 192, 'Three:': 193, 'three.': 194, 'Garden': 195, 'God,': 196, 'Chronopolis,': 197, 'Fates,': 198, 'Jellyfish': 199, 'sea,': 200, 'Burning': 201, 'Orphange,': 202, \"Dragon's\": 203, 'Prayer,': 204, 'Tower': 205, 'Stars,': 206, 'Dragon': 207, 'Radical': 208, 'Dreamers': 209, 'Unstealable': 210, 'Jewel.Overall,': 211, 'excellent': 212, 'should': 213, 'brought': 214, 'by': 215, 'those': 216, 'music.Xander': 217, 'Remember,': 218, 'Pull': 219, 'Your': 220, 'Jaw': 221, 'Off': 222, 'Floor': 223, 'After': 224, 'Hearing': 225, 'it:': 226, 'If': 227, 'know': 228, 'how': 229, 'divine': 230, 'is!': 231, 'Every': 232, 'single': 233, 'song': 234, 'tells': 235, 'story': 236, 'good!': 237, 'greatest': 238, 'songs': 239, 'without': 240, 'doubt,': 241, 'Cross:': 242, 'Scar,': 243, 'Magical': 244, 'Dreamers:': 245, 'Wind,': 246, 'Sea': 247, 'Unstolen': 248, 'Jewel.': 249, '(Translation': 250, 'varies)': 251, 'perfect': 252, 'ask': 253, 'me,': 254, 'can': 255, 'be.': 256, 'Mitsuda': 257, 'just': 258, 'poured': 259, 'his': 260, 'heart': 261, 'wrote': 262, 'down': 263, 'paper.': 264, 'absolute': 265, 'masterpiece:': 266, 'am': 267, 'quite': 268, 'sure': 269, 'actually': 270, 'taking': 271, 'time': 272, 'read': 273, 'at': 274, 'least': 275, 'once,': 276, 'few': 277, 'here.': 278, 'And': 279, 'whether': 280, 'were': 281, 'aware': 282, 'or': 283, 'not,': 284, 'contributed': 285, 'greatly': 286, 'mood': 287, 'minute': 288, 'whole': 289, 'game.Composed': 290, '3': 291, 'CDs': 292, 'exact': 293, 'count),': 294, 'which': 295, 'heart-rendering': 296, 'impressively': 297, 'remarkable,': 298, 'assure': 299, 'will': 300, 'not': 301, 'forget.': 302, 'It': 303, 'has': 304, 'everything': 305, 'listener': 306, '--': 307, 'from': 308, 'fast-paced': 309, 'energetic': 310, '(Dancing': 311, 'Tokage': 312, 'Termina': 313, 'Home),': 314, 'slower': 315, 'haunting': 316, '(Dragon': 317, 'God),': 318, 'purely': 319, 'beautifully': 320, 'composed': 321, \"(Time's\": 322, 'Scar),': 323, 'some': 324, 'fantastic': 325, 'vocals': 326, '(Radical': 327, 'Dreamers).This': 328, 'videogame': 329, 'soundtracks': 330, 'out': 331, 'there,': 332, 'surely': 333, 'ever.': 334, '^_^': 335, 'Buyer': 336, 'beware:': 337, 'self-published': 338, 'book,': 339, 'want': 340, 'why--read': 341, 'paragraphs!': 342, 'Those': 343, '5': 344, 'star': 345, 'written': 346, 'Ms.': 347, \"Haddon's\": 348, 'family': 349, 'friends--or': 350, 'perhaps,': 351, 'herself!': 352, 'imagine': 353, 'anyone': 354, 'thing--I': 355, 'spent': 356, 'evening': 357, 'with': 358, 'book': 359, 'friend': 360, 'we': 361, 'hysterics': 362, 'bits': 363, 'pieces': 364, 'another.': 365, 'definitely': 366, 'bad': 367, 'enough': 368, 'entered': 369, 'into': 370, 'kind': 371, '\"worst': 372, 'book\"': 373, 'contest.': 374, 'believe': 375, 'Amazon': 376, 'sells': 377, 'thing.': 378, 'Maybe': 379, 'offer': 380, 'them': 381, '8th': 382, 'grade': 383, 'term': 384, 'paper': 385, '\"To': 386, 'Kill': 387, 'Mockingbird\"--a': 388, 'Haddon': 389, 'of.': 390, 'Anyway,': 391, 'unless': 392, 'send': 393, 'someone': 394, 'joke---stay': 395, 'far,': 396, 'far': 397, 'away': 398, 'one!': 399, 'Glorious': 400, 'story:': 401, 'loved': 402, 'Whisper': 403, 'wicked': 404, 'saints.': 405, 'was': 406, 'pleasantly': 407, 'surprised': 408, 'changes': 409, 'book.': 410, 'normaly': 411, 'romance': 412, 'novels,': 413, 'world': 414, 'raving': 415, 'about': 416, 'bought': 417, 'it.': 418, '!!': 419, 'brilliant': 420, 'because': 421, 'true.': 422, 'wonderful': 423, 'told': 424, 'friends': 425, 'typical': 426, 'romance,': 427, 'more.': 428, 'Not': 429, 'crime,': 430, 'becuase': 431, 'missing': 432, 'warming': 433, 'story.': 434, 'A': 435, 'FIVE': 436, 'STAR': 437, 'BOOK:': 438, 'finished': 439, 'Wicked': 440, 'fell': 441, 'love': 442, 'caracters.': 443, 'expected': 444, 'average': 445, 'read,': 446, 'instead': 447, 'found': 448, 'books': 449, 'time.': 450, 'Just': 451, 'when': 452, 'thought': 453, 'predict': 454, 'outcome': 455, 'shocked': 456, '!': 457, 'writting': 458, 'descriptive': 459, 'broke': 460, \"Julia's\": 461, 'did': 462, 'felt': 463, 'there': 464, 'distant': 465, 'reader.': 466, 'lover': 467, 'novels': 468, 'then': 469, 'read.': 470, \"Don't\": 471, 'let': 472, 'cover': 473, 'fool': 474, 'spectacular!': 475, 'Whispers': 476, 'Saints:': 477, 'easy': 478, 'made': 479, 'keep': 480, 'on,': 481, 'put': 482, 'down.It': 483, 'left': 484, 'wanting': 485, 'follow': 486, 'coming': 487, 'soon.': 488, 'used': 489, 'gotten': 490, 'again.': 491, 'Very': 492, 'enjoyable.': 493, 'Worst!:': 494, 'complete': 495, 'waste': 496, 'Typographical': 497, 'errors,': 498, 'poor': 499, 'grammar,': 500, 'totally': 501, 'pathetic': 502, 'plot': 503, 'add': 504, 'up': 505, 'nothing.': 506, 'embarrassed': 507, 'author': 508, 'very': 509, 'disappointed': 510, 'paid': 511, 'Great': 512, 'book:': 513, 'great': 514, 'book,I': 515, 'down,and': 516, 'fast': 517, 'enough.': 518, 'Boy': 519, 'what': 520, 'twist': 521, 'turns': 522, 'keeps': 523, 'guessing': 524, 'happen': 525, 'next.': 526, 'makes': 527, 'fall': 528, 'heat': 529, 'up,it': 530, 'also': 531, 'make': 532, 'angery.': 533, 'go': 534, 'throu': 535, 'several': 536, 'your': 537, 'emotions.': 538, 'quick': 539, 'romance.': 540, 'something': 541, 'end': 542, 'day': 543, 'off': 544, 'night.': 545, 'Read:': 546, 'brilliant,': 547, 'yet': 548, 'realistic.': 549, 'showed': 550, 'error': 551, 'human.': 552, 'fact': 553, 'writer': 554, 'loving': 555, 'side': 556, 'God': 557, 'revengeful': 558, 'him.': 559, 'twisted': 560, 'turned': 561, 'glass': 562, 'castle.': 563, 'Oh': 564, 'please:': 565, 'guess': 566, 'novel': 567, 'one,': 568, 'discerning': 569, 'All': 570, 'others': 571, 'beware!': 572, 'drivel.': 573, 'trouble': 574, 'typo': 575, 'prominently': 576, 'featured': 577, 'back': 578, 'cover,': 579, 'first': 580, 'page': 581, 'removed': 582, 'doubt.': 583, 'Wait': 584, 'maybe': 585, 'point.': 586, 're-read': 587, 'beginning': 588, 'clear.': 589, 'intentional': 590, 'churning': 591, 'over-heated': 592, 'prose': 593, 'satiric': 594, 'purposes.': 595, 'Phew,': 596, 'glad': 597, \"didn't\": 598, '$10.95': 599, 'after': 600, 'all.': 601, 'Awful': 602, 'beyond': 603, 'belief!:': 604, 'wasting': 605, 'their': 606, 'money.': 607, 'seems': 608, '7th': 609, 'grader': 610, 'grammatical': 611, 'skills': 612, 'her': 613, 'age!': 614, 'As': 615, 'another': 616, 'reviewer': 617, 'points': 618, 'out,': 619, 'misspelling': 620, 'per': 621, 'chapter.': 622, 'For': 623, 'example,': 624, 'mentioned': 625, 'she': 626, 'had': 627, '\"lean\"': 628, 'house.': 629, 'distracted': 630, 'writing': 631, 'weak': 632, 'plot,': 633, 'decided': 634, 'pencil': 635, 'hand': 636, 'mark': 637, 'horrible': 638, 'grammar': 639, 'spelling.': 640, 'Please': 641, \"don't\": 642, 'too,': 643, 'good': 644, \"author's\": 645, 'relatives.': 646, 'faith': 647, 'on!': 648, 'try': 649, 'us': 650, 'fake': 651, 'reviews.:': 652, \"It's\": 653, 'glaringly': 654, 'obvious': 655, 'glowing': 656, 'same': 657, 'person,': 658, 'perhaps': 659, 'herself.': 660, 'They': 661, 'misspellings': 662, 'sentence': 663, 'structure': 664, 'Who': 665, 'Veronica': 666, 'think': 667, 'author?': 668, 'romantic': 669, 'zen': 670, 'baseball': 671, 'comedy:': 672, 'When': 673, 'hear': 674, 'folks': 675, 'they': 676, \"'em\": 677, 'anymore,': 678, 'might': 679, 'talking': 680, '\"BY': 681, 'THE': 682, 'SEA\".': 683, 'cool': 684, 'young': 685, 'Cuban': 686, 'girl': 687, 'searching': 688, 'idenity': 689, 'stumbles': 690, 'coastal': 691, 'resort': 692, 'kitchen': 693, 'gig': 694, 'motorcycle': 695, 'maintenance': 696, 'man,': 697, 'three': 698, 'hysterical': 699, 'Italian': 700, 'chefs': 701, 'Latino': 702, 'fireballing': 703, 'right': 704, 'handed': 705, 'pitcher': 706, 'plays': 707, 'team': 708, 'sponsored': 709, \"resort's\": 710, 'owner.': 711, 'often': 712, 'case': 713, \"'finds'\": 714, 'herself': 715, 'through': 716, 'honest,': 717, 'comical': 718, 'always': 719, 'emotional,': 720, 'interaction': 721, 'sizzling': 722, 'roster': 723, 'players.': 724, 'With': 725, 'mix': 726, 'special': 727, 'effects,': 728, 'salsa': 729, 'sound': 730, 'flashbacks,': 731, 'BY': 732, 'SEA,': 733, 'gets': 734, '4': 735, 'BIG': 736, 'stars': 737, 'me!': 738, 'Fashionable': 739, 'Compression': 740, 'Stockings!:': 741, 'DVT': 742, 'doctor': 743, 'required': 744, 'wear': 745, 'compression': 746, 'stockings.': 747, 'wore': 748, 'ugly': 749, 'white': 750, 'TED': 751, 'hose': 752, 'yucky': 753, 'thick': 754, 'brown': 755, 'Then': 756, 'Jobst': 757, 'UltraSheer.': 758, 'gave': 759, 'needed': 760, '(15-20,)': 761, 'looked': 762, 'regular': 763, 'pantyhose.': 764, 'Even': 765, 'though': 766, 'blood': 767, 'clot': 768, 'gone': 769, 'years,': 770, 'still': 771, 'these': 772, 'support': 773, 'stockings': 774, 'legs': 775, 'nice.**Note,': 776, 'problems': 777, 'rubberized': 778, 'tops': 779, 'rolling': 780, 'thigh.': 781, 'tried': 782, 'adhesive,': 783, 'hated': 784, 'having': 785, 'skin': 786, 'pulled': 787, 'day.': 788, 'inexpensive': 789, 'garter': 790, 'belt': 791, 'works': 792, 'fine': 793, 'helps': 794, 'rolling.': 795, 'UltraSheer': 796, 'Thigh': 797, 'High:': 798, 'product.': 799, 'However,': 800, 'difficult': 801, 'get': 802, 'older': 803, 'people.': 804, \"I've\": 805, 'full': 806, 'workout': 807, 'getting': 808, 'on.': 809, 'Also,': 810, 'wears': 811, 'begin': 812, 'roll': 813, 'top': 814, 'create': 815, 'deep': 816, 'ridge': 817, 'skin.': 818, 'them,': 819, 'two': 820, 'difficulties': 821, 'addressed': 822, 'such': 823, 'help.': 824, 'sizes': 825, 'recomended': 826, 'size': 827, 'chart': 828, 'real:': 829, 'smaller': 830, 'than': 831, 'chart.': 832, 'sheer': 833, 'it!.': 834, 'item': 835, 'internet..it': 836, 'better': 837, 'store': 838, 'check': 839, 'mens': 840, 'ultrasheer:': 841, 'model': 842, 'may': 843, 'ok': 844, 'sedentary': 845, 'types,': 846, 'active': 847, 'around': 848, 'alot': 849, 'job': 850, 'consistently': 851, 'rolled': 852, 'ankles!': 853, 'Good!!': 854, 'Solution:': 855, 'standard': 856, 'stocking,': 857, '20-30,': 858, 'stock': 859, '#114622.': 860, 'support,': 861, 'stays': 862, 'gives': 863, 'need.': 864, 'Both': 865, 'pair': 866, 'tore': 867, 'struggled': 868, 'pull': 869, 'Good': 870, 'riddance/bad': 871, 'investment!': 872, 'Delicious': 873, 'cookie': 874, 'mix:': 875, 'funny': 876, 'product': 877, 'knowing': 878, 'mix.': 879, 'header': 880, 'quickly': 881, 'packaged': 882, 'cookies.': 883, 'But': 884, 'no,': 885, 'MIX': 886, 'noticed': 887, 'since': 888, 'title.This': 889, 'baking': 890, 'convenience': 891, 'dough': 892, 'wrapped': 893, 'plastic': 894, 'logs': 895, 'bit': 896, 'surprise.': 897, 'Mixing': 898, 'VERY': 899, 'messy': 900, '(it': 901, 'extremely': 902, 'sticky).': 903, 'flexibility': 904, 'ratio': 905, 'ingredients': 906, 'extra': 907, 'butter': 908, 'baked': 909, 'cookies': 910, 'chewy).': 911, 'really': 912, 'large': 913, 'chocolate': 914, 'chips': 915, 'it--I': 916, 'that.I': 917, 'addition': 918, \"'natural\": 919, \"flavors'\": 920, 'Another': 921, 'Abysmal': 922, 'Digital': 923, 'Copy:': 924, 'Rather': 925, 'scratches': 926, 'insect': 927, 'droppings,': 928, 'random': 929, 'pixelations': 930, 'combined': 931, 'muddy': 932, 'light': 933, 'vague': 934, 'image': 935, 'resolution.': 936, 'Probably': 937, 'cue': 938, 'packaging': 939, 'straight': 940, 'street': 941, 'corner': 942, 'bootleg': 943, 'dealer.If': 944, 'seen': 945, 'reasonably': 946, 'condition': 947, 'film': 948, 'copy,': 949, 'defining': 950, 'visuals': 951, 'crystal': 952, 'clear': 953, 'lighting': 954, 'contrasts': 955, 'black': 956, 'white.': 957, 'surrounding': 958, 'countryside': 959, \"'old\": 960, \"home'\": 961, 'scenes': 962, 'set': 963, 'early': 964, 'morning': 965, 'ground': 966, 'mists': 967, 'haze': 968, 'memory': 969, 'while': 970, 'events': 971, 'bridge': 972, 'water': 973, 'bright,': 974, 'clear,': 975, 'immediate.Here': 976, 'dull,': 977, 'dark,': 978, 'clouded.': 979, 'Or,': 980, 'remember': 981, 'timbre': 982, 'enunciation': 983, \"Captain's\": 984, 'commands,': 985, 'visuals.After': 986, 'that,': 987, 'hard': 988, 'award': 989, 'winning,': 990, 'critically': 991, 'acclaimed': 992, \"film's\": 993, 'presentation': 994, 'YOUTUBE.': 995, 'Somewhere': 996, '\"out': 997, 'there\"': 998, 'DVD': 999, 'comes': 1000, '16mm': 1001, 'public': 1002, 'library': 1003, 'reel.Just': 1004, 'none': 1005, 'appear': 1006, 'Amazon.': 1007, 'fascinating': 1008, 'insight': 1009, 'life': 1010, 'modern': 1011, 'Japanese': 1012, 'teens:': 1013, 'thoroughly': 1014, 'enjoyed': 1015, 'Rising': 1016, 'Sons': 1017, 'Daughters.': 1018, 'other': 1019, 'looks': 1020, 'society': 1021, 'point': 1022, 'view': 1023, 'people': 1024, 'poised': 1025, 'between': 1026, \"parents'\": 1027, 'age-old': 1028, 'culture': 1029, 'restraint': 1030, 'obedience': 1031, 'community,': 1032, \"peers'\": 1033, 'adulation': 1034, 'Western': 1035, 'culture.': 1036, 'True': 1037, 'form,': 1038, '\"New': 1039, 'Young\"': 1040, 'Japan': 1041, 'seem': 1042, 'creating': 1043, '\"international\"': 1044, 'blend,': 1045, 'Ando': 1046, 'demonstrates': 1047, 'vignettes': 1048, 'private': 1049, 'lives': 1050, 'members': 1051, 'family.': 1052, 'Steven': 1053, 'Wardell': 1054, 'clearly': 1055, 'talented': 1056, 'author,': 1057, 'adopted': 1058, 'schooling': 1059, 'four': 1060, 'teens,': 1061, 'thus': 1062, 'able': 1063, 'inside': 1064, 'out.': 1065, 'read!': 1066, 'i': 1067, 'liked': 1068, 'album': 1069, 'would:': 1070, 'o': 1071, 'o,but': 1072, 'listened': 1073, '\"blue': 1074, 'angel\",\"lanna\"': 1075, '\\'mama\"': 1076, 'hair': 1077, 'rose': 1078, 'neck.Roy': 1079, 'trully': 1080, 'singer': 1081, 'talent': 1082, 'find': 1083, 'days.': 1084, 'Problem': 1085, 'charging': 1086, 'AAAs:': 1087, 'charger': 1088, 'charges': 1089, 'AA': 1090, 'batteries': 1091, 'fine,': 1092, 'huge': 1093, 'problem': 1094, 'securing': 1095, 'AAA': 1096, 'batteries.': 1097, 'To': 1098, 'charge': 1099, 'need': 1100, 'flip': 1101, 'little': 1102, 'button': 1103, 'positive': 1104, 'end.': 1105, 'In': 1106, 'pop': 1107, 'up,': 1108, \"won't\": 1109, 'hold.': 1110, 'mechanism': 1111, 'became': 1112, 'loose,': 1113, 'horizontal': 1114, 'pressure': 1115, 'push': 1116, 'buttons': 1117, 'up.': 1118, 'What': 1119, 'do': 1120, 'using': 1121, 'duct': 1122, 'tape': 1123, 'segment': 1124, 'crayon,': 1125, 'apply': 1126, 'crayon': 1127, 'buttons,': 1128, 'wrap': 1129, 'around.': 1130, 'You': 1131, 'painful': 1132, 'is.': 1133, 'Works,': 1134, 'advertised:': 1135, 'chargers..the': 1136, 'instructions': 1137, 'lights': 1138, 'stay': 1139, 'battery': 1140, 'charges...true.': 1141, 'doNT': 1142, 'turn': 1143, 'done.': 1144, 'Which': 1145, '24': 1146, 'hours': 1147, 'returned': 1148, 'thinking': 1149, 'unit.The': 1150, 'new': 1151, 'kept': 1152, 'does': 1153, 'charge...but': 1154, 'useless': 1155, '\"backup\"': 1156, 'manage': 1157, 'drain': 1158, 'AAs': 1159, \"wouldn't\": 1160, 'charger.': 1161, 'Disappointed:': 1162, 'reviews,made': 1163, 'purchase': 1164, 'disappointed.': 1165, 'convenient': 1166, 'once': 1167, 'lasts': 1168, 'short': 1169, 'longer': 1170, 'kodak': 1171, 'NiMH': 1172, 'dear:': 1173, 'excited': 1174, 'ostensibly': 1175, 'Muslim': 1176, 'feminism,': 1177, 'volume': 1178, 'live': 1179, 'expectations.One': 1180, 'essay,': 1181, 'among': 1182, 'things,': 1183, 'describes': 1184, 'veil': 1185, 'potentially': 1186, 'liberating.': 1187, \"doesn't\": 1188, 'explain': 1189, 'why.Another,': 1190, 'women': 1191, 'Cape': 1192, 'Town,': 1193, 'claims': 1194, 'separate': 1195, '\"more': 1196, 'equal.\"': 1197, 'Gee': 1198, 'whiz,': 1199, 'disappointment.I': 1200, 'hoped': 1201, 'feminist': 1202, 'condemnation': 1203, 'gender': 1204, 'apartheid.': 1205, \"book.I'm\": 1206, 'essay': 1207, 'extolling': 1208, 'virtues': 1209, 'female': 1210, 'genital': 1211, 'mutilation.--Alyssa': 1212, 'A.': 1213, 'Lappen': 1214, 'Based': 1215, 'did!:': 1216, 'VCR/DVD': 1217, 'Christmas': 1218, 'present': 1219, 'myself': 1220, 'deciding': 1221, 'join': 1222, 'rest': 1223, 'DVD-land': 1224, 'VHS': 1225, 'movies': 1226, 'yet.': 1227, 'price,': 1228, 'own': 1229, 'JVC': 1230, 'TV,': 1231, 'choice.': 1232, 'agree': 1233, 'set-up.': 1234, 'awkward': 1235, 'TV/VHS/DVD': 1236, 'selection': 1237, 'options': 1238, 'hang': 1239, 'it.Two': 1240, 'comments:': 1241, 'intuitive': 1242, 'complicated': 1243, '(too': 1244, 'many': 1245, 'remote': 1246, 'please': 1247, 'technically-minded': 1248, 'me)': 1249, 'rely': 1250, 'heavily': 1251, 'how-to': 1252, 'manual.': 1253, 'setting': 1254, 'VCR': 1255, 'timer': 1256, 'enter': 1257, 'start': 1258, 'scroll': 1259, '(unless': 1260, 'something)...but': 1261, 'complaints.': 1262, '$$$.': 1263, 'Incorrect': 1264, 'disc!:': 1265, 'big': 1266, 'fan,': 1267, 'model,': 1268, 'suspiscious': 1269, 'saw': 1270, 'units': 1271, 'return': 1272, 'section': 1273, 'store.': 1274, 'anyway': 1275, '(new)': 1276, 'happy.': 1277, 'unit': 1278, 'sends': 1279, 'clicks': 1280, 'receiver': 1281, 'while,': 1282, 'transition': 1283, 'smooth,(like': 1284, 'pause)': 1285, 'fairly': 1286, 'DVD,CD': 1287, 'headcleaner': 1288, 'work.': 1289, '\"incorrect': 1290, 'disc\"': 1291, 'message.': 1292, 'happy': 1293, 'it...but:': 1294, 'nut...I': 1295, 'televisions,': 1296, 'VCR,': 1297, 'bookshelf': 1298, 'audio': 1299, 'system': 1300, 'car': 1301, 'system.': 1302, 'So': 1303, 'came': 1304, 'move': 1305, 'player': 1306, 'boys': 1307, 'room': 1308, 'old': 1309, 'man': 1310, 'knew': 1311, 'combo': 1312, 'longer.': 1313, 'except': 1314, '2': 1315, 'things:(1)no': 1316, 'cable': 1317, 'box': 1318, 'compatability': 1319, 'control': 1320, '(2)no': 1321, 'seperate': 1322, 'inputs': 1323, 'input': 1324, 'coax': 1325, 'programming': 1326, 'mono...wife': 1327, 'tell': 1328, 'difference': 1329, \"she's\": 1330, \"I'll\": 1331, 'look': 1332, 'GREAT!!!!!!!': 1333, 'titled': 1334, '\"Hollywood': 1335, 'Debacle\":': 1336, 'ridiculous,': 1337, 'wonder': 1338, 'script': 1339, 'before': 1340, 'making': 1341, 'film.': 1342, 'mountain': 1343, 'lion': 1344, 'breaks': 1345, 'trailer': 1346, 'cars': 1347, 'behind': 1348, 'notice?': 1349, 'captured': 1350, 'jail': 1351, 'cell?': 1352, 'Get': 1353, 'real!': 1354, 'Utterly,': 1355, 'completely': 1356, 'stupid.': 1357, 'Is': 1358, 'TV???': 1359, 'bet': 1360, 'is:': 1361, 'Hotel': 1362, 'Babylon': 1363, \"TV...it's\": 1364, 'TV!!!!': 1365, 'show': 1366, 'features': 1367, 'incredible': 1368, 'acting': 1369, 'Tamzin': 1370, 'Outhwaite': 1371, '(formerly': 1372, 'EastEnders,': 1373, 'BBC': 1374, 'soap)': 1375, 'Max': 1376, 'Beesley': 1377, '(from': 1378, 'ill-fated': 1379, 'movie': 1380, '\"Glitter\"': 1381, 'starring': 1382, 'Mariah': 1383, 'Carey).': 1384, 'drama': 1385, 'series,': 1386, 'drama,': 1387, 'comedy,': 1388, 'soap': 1389, 'opera': 1390, 'mixed': 1391, 'show.': 1392, 'aired': 1393, 'America': 1394, 'seeing': 1395, 'got': 1396, 'episodes': 1397, 'great.': 1398, 'season': 1399, 'finale': 1400, 'interesting': 1401, 'watch.The': 1402, 'reminds': 1403, 'ABC': 1404, '1983': 1405, '1988.': 1406, 'reason...Hotel': 1407, 'fictional': 1408, 'San': 1409, 'Francisco': 1410, 'hotel': 1411, 'luxury': 1412, 'five-star': 1413, 'England.I': 1414, 'recommend': 1415, 'willing': 1416, 'watch': 1417, 'BBC.': 1418, 'Nothing': 1419, 'already': 1420, 'know:': 1421, 'casually': 1422, 'applying': 1423, 'law': 1424, 'school,': 1425, 'seriously!!': 1426, 'Unfortunately': 1427, \"wasn't\": 1428, 'entertaining': 1429, 'bit.:': 1430, 'ordered': 1431, 'CD,': 1432, 'hip,': 1433, 'daddy': 1434, 'vibe': 1435, 'CD.': 1436, 'However': 1437, 'dismay': 1438, 'sounds': 1439, 'fourth': 1440, 'class.': 1441, 'main': 1442, 'jist': 1443, 'CD': 1444, 'xylaphone': 1445, 'playing': 1446, 'over': 1447, 'peoples': 1448, 'voices': 1449, 'trying': 1450, 'replicate': 1451, 'happening': 1452, 'party.': 1453, 'party': 1454, 'anywhere': 1455, 'neighborhood': 1456, 'laughed': 1457, 'beach.': 1458, 'Growing': 1459, 'surfer': 1460, 'Diego,': 1461, 'Southern': 1462, 'California': 1463, 'brothers.': 1464, 'Honestly,': 1465, 'kinda': 1466, 'B': 1467, 'movie.': 1468, 'absolutle': 1469, 'epitimy': 1470, 'last': 1471, '\"vibe\"': 1472, 'Surf': 1473, 'Cha': 1474, 'Cha.': 1475, 'Surfers': 1476, 'CHA.': 1477, 'Rochelle': 1478, 'explains': 1479, 'You:': 1480, 'Wondering': 1481, 'hell': 1482, 'happened': 1483, 'moral': 1484, 'aspect': 1485, 'American': 1486, '?': 1487, 'lucid,': 1488, 'well': 1489, 'argued': 1490, 'explanation': 1491, 'simple': 1492, 'become': 1493, 'focused': 1494, 'our': 1495, 'individual': 1496, 'RIGHTS': 1497, 'ignored,': 1498, 'mocked,': 1499, 'personal': 1500, 'responsibilities.': 1501, 'final': 1502, 'response': 1503, 'indictment': 1504, 'Robert': 1505, \"Ringer's\": 1506, 'seller,': 1507, 'LOOKING': 1508, 'OUT': 1509, 'FOR': 1510, '#1.': 1511, 'disgusted': 1512, 'boorish': 1513, 'state': 1514, 'media,': 1515, 'politics': 1516, 'discourse': 1517, 'general,': 1518, 'heads': 1519, 'substantial': 1520, 'challenges': 1521, 'lie': 1522, 'Americans,': 1523, 'human': 1524, 'beings.': 1525}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVEmkKcrFQf7",
        "colab_type": "text"
      },
      "source": [
        "2) Also, provide one-hot coding using Keras built-in function (listing 6.3)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kO-JIxouFqeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "samples = texts[:36] # using only first 36 reviews"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdVy7u8rFJJI",
        "colab_type": "code",
        "outputId": "b85a0b99-5bce-49fc-9314-4e4ca0a2aa34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "#samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "tokenizer = Tokenizer(num_words=1000)\n",
        "tokenizer.fit_on_texts(samples)\n",
        "sequences = tokenizer.texts_to_sequences(samples)\n",
        "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1229 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBUb6ThzFo_4",
        "colab_type": "code",
        "outputId": "642d3715-a326-4433-ee8e-60471f94178f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(token_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'The': 1, 'best': 2, 'soundtrack': 3, 'ever': 4, 'to': 5, 'anything.:': 6, \"I'm\": 7, 'reading': 8, 'a': 9, 'lot': 10, 'of': 11, 'reviews': 12, 'saying': 13, 'that': 14, 'this': 15, 'is': 16, 'the': 17, \"'game\": 18, \"soundtrack'\": 19, 'and': 20, 'I': 21, 'figured': 22, \"I'd\": 23, 'write': 24, 'review': 25, 'disagree': 26, 'bit.': 27, 'This': 28, 'in': 29, 'my': 30, 'opinino': 31, 'Yasunori': 32, \"Mitsuda's\": 33, 'ultimate': 34, 'masterpiece.': 35, 'music': 36, 'timeless': 37, 'been': 38, 'listening': 39, 'it': 40, 'for': 41, 'years': 42, 'now': 43, 'its': 44, 'beauty': 45, 'simply': 46, 'refuses': 47, 'fade.The': 48, 'price': 49, 'tag': 50, 'on': 51, 'pretty': 52, 'staggering': 53, 'must': 54, 'say,': 55, 'but': 56, 'if': 57, 'you': 58, 'are': 59, 'going': 60, 'buy': 61, 'any': 62, 'cd': 63, 'much': 64, 'money,': 65, 'only': 66, 'one': 67, 'feel': 68, 'would': 69, 'be': 70, 'worth': 71, 'every': 72, 'penny.': 73, 'Amazing!:': 74, 'favorite': 75, 'all': 76, 'time,': 77, 'hands': 78, 'down.': 79, 'intense': 80, 'sadness': 81, '\"Prisoners': 82, 'Fate\"': 83, '(which': 84, 'means': 85, 'more': 86, \"you've\": 87, 'played': 88, 'game)': 89, 'hope': 90, '\"A': 91, 'Distant': 92, 'Promise\"': 93, '\"Girl': 94, 'who': 95, 'Stole': 96, 'Star\"': 97, 'have': 98, 'an': 99, 'important': 100, 'inspiration': 101, 'me': 102, 'personally': 103, 'throughout': 104, 'teen': 105, 'years.': 106, 'higher': 107, 'energy': 108, 'tracks': 109, 'like': 110, '\"Chrono': 111, 'Cross': 112, '~': 113, \"Time's\": 114, 'Scar~\",': 115, '\"Time': 116, 'Dreamwatch\",': 117, '\"Chronomantique\"': 118, '(indefinably': 119, 'remeniscent': 120, 'Chrono': 121, 'Trigger)': 122, 'absolutely': 123, 'superb': 124, 'as': 125, 'well.This': 126, 'amazing': 127, 'music,': 128, 'probably': 129, \"composer's\": 130, 'work': 131, '(I': 132, \"haven't\": 133, 'heard': 134, 'Xenogears': 135, 'soundtrack,': 136, 'so': 137, \"can't\": 138, 'say': 139, 'sure),': 140, 'even': 141, 'never': 142, 'game,': 143, 'twice': 144, 'it.I': 145, 'wish': 146, 'could': 147, 'give': 148, '6': 149, 'stars.': 150, 'Excellent': 151, 'Soundtrack:': 152, 'truly': 153, 'enjoy': 154, 'video': 155, 'game': 156, 'music.': 157, 'most': 158, 'here': 159, \"it's\": 160, 'relaxing': 161, 'peaceful.On': 162, 'disk': 163, 'one.': 164, 'favorites': 165, 'Scars': 166, 'Of': 167, 'Time,': 168, 'Between': 169, 'Life': 170, 'Death,': 171, 'Forest': 172, 'Illusion,': 173, 'Fortress': 174, 'Ancient': 175, 'Dragons,': 176, 'Lost': 177, 'Fragment,': 178, 'Drowned': 179, 'Valley.Disk': 180, 'Two:': 181, 'Draggons,': 182, 'Galdorb': 183, '-': 184, 'Home,': 185, 'Chronomantique,': 186, 'Prisoners': 187, 'Fate,': 188, 'Gale,': 189, 'girlfriend': 190, 'likes': 191, 'ZelbessDisk': 192, 'Three:': 193, 'three.': 194, 'Garden': 195, 'God,': 196, 'Chronopolis,': 197, 'Fates,': 198, 'Jellyfish': 199, 'sea,': 200, 'Burning': 201, 'Orphange,': 202, \"Dragon's\": 203, 'Prayer,': 204, 'Tower': 205, 'Stars,': 206, 'Dragon': 207, 'Radical': 208, 'Dreamers': 209, 'Unstealable': 210, 'Jewel.Overall,': 211, 'excellent': 212, 'should': 213, 'brought': 214, 'by': 215, 'those': 216, 'music.Xander': 217, 'Remember,': 218, 'Pull': 219, 'Your': 220, 'Jaw': 221, 'Off': 222, 'Floor': 223, 'After': 224, 'Hearing': 225, 'it:': 226, 'If': 227, 'know': 228, 'how': 229, 'divine': 230, 'is!': 231, 'Every': 232, 'single': 233, 'song': 234, 'tells': 235, 'story': 236, 'good!': 237, 'greatest': 238, 'songs': 239, 'without': 240, 'doubt,': 241, 'Cross:': 242, 'Scar,': 243, 'Magical': 244, 'Dreamers:': 245, 'Wind,': 246, 'Sea': 247, 'Unstolen': 248, 'Jewel.': 249, '(Translation': 250, 'varies)': 251, 'perfect': 252, 'ask': 253, 'me,': 254, 'can': 255, 'be.': 256, 'Mitsuda': 257, 'just': 258, 'poured': 259, 'his': 260, 'heart': 261, 'wrote': 262, 'down': 263, 'paper.': 264, 'absolute': 265, 'masterpiece:': 266, 'am': 267, 'quite': 268, 'sure': 269, 'actually': 270, 'taking': 271, 'time': 272, 'read': 273, 'at': 274, 'least': 275, 'once,': 276, 'few': 277, 'here.': 278, 'And': 279, 'whether': 280, 'were': 281, 'aware': 282, 'or': 283, 'not,': 284, 'contributed': 285, 'greatly': 286, 'mood': 287, 'minute': 288, 'whole': 289, 'game.Composed': 290, '3': 291, 'CDs': 292, 'exact': 293, 'count),': 294, 'which': 295, 'heart-rendering': 296, 'impressively': 297, 'remarkable,': 298, 'assure': 299, 'will': 300, 'not': 301, 'forget.': 302, 'It': 303, 'has': 304, 'everything': 305, 'listener': 306, '--': 307, 'from': 308, 'fast-paced': 309, 'energetic': 310, '(Dancing': 311, 'Tokage': 312, 'Termina': 313, 'Home),': 314, 'slower': 315, 'haunting': 316, '(Dragon': 317, 'God),': 318, 'purely': 319, 'beautifully': 320, 'composed': 321, \"(Time's\": 322, 'Scar),': 323, 'some': 324, 'fantastic': 325, 'vocals': 326, '(Radical': 327, 'Dreamers).This': 328, 'videogame': 329, 'soundtracks': 330, 'out': 331, 'there,': 332, 'surely': 333, 'ever.': 334, '^_^': 335, 'Buyer': 336, 'beware:': 337, 'self-published': 338, 'book,': 339, 'want': 340, 'why--read': 341, 'paragraphs!': 342, 'Those': 343, '5': 344, 'star': 345, 'written': 346, 'Ms.': 347, \"Haddon's\": 348, 'family': 349, 'friends--or': 350, 'perhaps,': 351, 'herself!': 352, 'imagine': 353, 'anyone': 354, 'thing--I': 355, 'spent': 356, 'evening': 357, 'with': 358, 'book': 359, 'friend': 360, 'we': 361, 'hysterics': 362, 'bits': 363, 'pieces': 364, 'another.': 365, 'definitely': 366, 'bad': 367, 'enough': 368, 'entered': 369, 'into': 370, 'kind': 371, '\"worst': 372, 'book\"': 373, 'contest.': 374, 'believe': 375, 'Amazon': 376, 'sells': 377, 'thing.': 378, 'Maybe': 379, 'offer': 380, 'them': 381, '8th': 382, 'grade': 383, 'term': 384, 'paper': 385, '\"To': 386, 'Kill': 387, 'Mockingbird\"--a': 388, 'Haddon': 389, 'of.': 390, 'Anyway,': 391, 'unless': 392, 'send': 393, 'someone': 394, 'joke---stay': 395, 'far,': 396, 'far': 397, 'away': 398, 'one!': 399, 'Glorious': 400, 'story:': 401, 'loved': 402, 'Whisper': 403, 'wicked': 404, 'saints.': 405, 'was': 406, 'pleasantly': 407, 'surprised': 408, 'changes': 409, 'book.': 410, 'normaly': 411, 'romance': 412, 'novels,': 413, 'world': 414, 'raving': 415, 'about': 416, 'bought': 417, 'it.': 418, '!!': 419, 'brilliant': 420, 'because': 421, 'true.': 422, 'wonderful': 423, 'told': 424, 'friends': 425, 'typical': 426, 'romance,': 427, 'more.': 428, 'Not': 429, 'crime,': 430, 'becuase': 431, 'missing': 432, 'warming': 433, 'story.': 434, 'A': 435, 'FIVE': 436, 'STAR': 437, 'BOOK:': 438, 'finished': 439, 'Wicked': 440, 'fell': 441, 'love': 442, 'caracters.': 443, 'expected': 444, 'average': 445, 'read,': 446, 'instead': 447, 'found': 448, 'books': 449, 'time.': 450, 'Just': 451, 'when': 452, 'thought': 453, 'predict': 454, 'outcome': 455, 'shocked': 456, '!': 457, 'writting': 458, 'descriptive': 459, 'broke': 460, \"Julia's\": 461, 'did': 462, 'felt': 463, 'there': 464, 'distant': 465, 'reader.': 466, 'lover': 467, 'novels': 468, 'then': 469, 'read.': 470, \"Don't\": 471, 'let': 472, 'cover': 473, 'fool': 474, 'spectacular!': 475, 'Whispers': 476, 'Saints:': 477, 'easy': 478, 'made': 479, 'keep': 480, 'on,': 481, 'put': 482, 'down.It': 483, 'left': 484, 'wanting': 485, 'follow': 486, 'coming': 487, 'soon.': 488, 'used': 489, 'gotten': 490, 'again.': 491, 'Very': 492, 'enjoyable.': 493, 'Worst!:': 494, 'complete': 495, 'waste': 496, 'Typographical': 497, 'errors,': 498, 'poor': 499, 'grammar,': 500, 'totally': 501, 'pathetic': 502, 'plot': 503, 'add': 504, 'up': 505, 'nothing.': 506, 'embarrassed': 507, 'author': 508, 'very': 509, 'disappointed': 510, 'paid': 511, 'Great': 512, 'book:': 513, 'great': 514, 'book,I': 515, 'down,and': 516, 'fast': 517, 'enough.': 518, 'Boy': 519, 'what': 520, 'twist': 521, 'turns': 522, 'keeps': 523, 'guessing': 524, 'happen': 525, 'next.': 526, 'makes': 527, 'fall': 528, 'heat': 529, 'up,it': 530, 'also': 531, 'make': 532, 'angery.': 533, 'go': 534, 'throu': 535, 'several': 536, 'your': 537, 'emotions.': 538, 'quick': 539, 'romance.': 540, 'something': 541, 'end': 542, 'day': 543, 'off': 544, 'night.': 545, 'Read:': 546, 'brilliant,': 547, 'yet': 548, 'realistic.': 549, 'showed': 550, 'error': 551, 'human.': 552, 'fact': 553, 'writer': 554, 'loving': 555, 'side': 556, 'God': 557, 'revengeful': 558, 'him.': 559, 'twisted': 560, 'turned': 561, 'glass': 562, 'castle.': 563, 'Oh': 564, 'please:': 565, 'guess': 566, 'novel': 567, 'one,': 568, 'discerning': 569, 'All': 570, 'others': 571, 'beware!': 572, 'drivel.': 573, 'trouble': 574, 'typo': 575, 'prominently': 576, 'featured': 577, 'back': 578, 'cover,': 579, 'first': 580, 'page': 581, 'removed': 582, 'doubt.': 583, 'Wait': 584, 'maybe': 585, 'point.': 586, 're-read': 587, 'beginning': 588, 'clear.': 589, 'intentional': 590, 'churning': 591, 'over-heated': 592, 'prose': 593, 'satiric': 594, 'purposes.': 595, 'Phew,': 596, 'glad': 597, \"didn't\": 598, '$10.95': 599, 'after': 600, 'all.': 601, 'Awful': 602, 'beyond': 603, 'belief!:': 604, 'wasting': 605, 'their': 606, 'money.': 607, 'seems': 608, '7th': 609, 'grader': 610, 'grammatical': 611, 'skills': 612, 'her': 613, 'age!': 614, 'As': 615, 'another': 616, 'reviewer': 617, 'points': 618, 'out,': 619, 'misspelling': 620, 'per': 621, 'chapter.': 622, 'For': 623, 'example,': 624, 'mentioned': 625, 'she': 626, 'had': 627, '\"lean\"': 628, 'house.': 629, 'distracted': 630, 'writing': 631, 'weak': 632, 'plot,': 633, 'decided': 634, 'pencil': 635, 'hand': 636, 'mark': 637, 'horrible': 638, 'grammar': 639, 'spelling.': 640, 'Please': 641, \"don't\": 642, 'too,': 643, 'good': 644, \"author's\": 645, 'relatives.': 646, 'faith': 647, 'on!': 648, 'try': 649, 'us': 650, 'fake': 651, 'reviews.:': 652, \"It's\": 653, 'glaringly': 654, 'obvious': 655, 'glowing': 656, 'same': 657, 'person,': 658, 'perhaps': 659, 'herself.': 660, 'They': 661, 'misspellings': 662, 'sentence': 663, 'structure': 664, 'Who': 665, 'Veronica': 666, 'think': 667, 'author?': 668, 'romantic': 669, 'zen': 670, 'baseball': 671, 'comedy:': 672, 'When': 673, 'hear': 674, 'folks': 675, 'they': 676, \"'em\": 677, 'anymore,': 678, 'might': 679, 'talking': 680, '\"BY': 681, 'THE': 682, 'SEA\".': 683, 'cool': 684, 'young': 685, 'Cuban': 686, 'girl': 687, 'searching': 688, 'idenity': 689, 'stumbles': 690, 'coastal': 691, 'resort': 692, 'kitchen': 693, 'gig': 694, 'motorcycle': 695, 'maintenance': 696, 'man,': 697, 'three': 698, 'hysterical': 699, 'Italian': 700, 'chefs': 701, 'Latino': 702, 'fireballing': 703, 'right': 704, 'handed': 705, 'pitcher': 706, 'plays': 707, 'team': 708, 'sponsored': 709, \"resort's\": 710, 'owner.': 711, 'often': 712, 'case': 713, \"'finds'\": 714, 'herself': 715, 'through': 716, 'honest,': 717, 'comical': 718, 'always': 719, 'emotional,': 720, 'interaction': 721, 'sizzling': 722, 'roster': 723, 'players.': 724, 'With': 725, 'mix': 726, 'special': 727, 'effects,': 728, 'salsa': 729, 'sound': 730, 'flashbacks,': 731, 'BY': 732, 'SEA,': 733, 'gets': 734, '4': 735, 'BIG': 736, 'stars': 737, 'me!': 738, 'Fashionable': 739, 'Compression': 740, 'Stockings!:': 741, 'DVT': 742, 'doctor': 743, 'required': 744, 'wear': 745, 'compression': 746, 'stockings.': 747, 'wore': 748, 'ugly': 749, 'white': 750, 'TED': 751, 'hose': 752, 'yucky': 753, 'thick': 754, 'brown': 755, 'Then': 756, 'Jobst': 757, 'UltraSheer.': 758, 'gave': 759, 'needed': 760, '(15-20,)': 761, 'looked': 762, 'regular': 763, 'pantyhose.': 764, 'Even': 765, 'though': 766, 'blood': 767, 'clot': 768, 'gone': 769, 'years,': 770, 'still': 771, 'these': 772, 'support': 773, 'stockings': 774, 'legs': 775, 'nice.**Note,': 776, 'problems': 777, 'rubberized': 778, 'tops': 779, 'rolling': 780, 'thigh.': 781, 'tried': 782, 'adhesive,': 783, 'hated': 784, 'having': 785, 'skin': 786, 'pulled': 787, 'day.': 788, 'inexpensive': 789, 'garter': 790, 'belt': 791, 'works': 792, 'fine': 793, 'helps': 794, 'rolling.': 795, 'UltraSheer': 796, 'Thigh': 797, 'High:': 798, 'product.': 799, 'However,': 800, 'difficult': 801, 'get': 802, 'older': 803, 'people.': 804, \"I've\": 805, 'full': 806, 'workout': 807, 'getting': 808, 'on.': 809, 'Also,': 810, 'wears': 811, 'begin': 812, 'roll': 813, 'top': 814, 'create': 815, 'deep': 816, 'ridge': 817, 'skin.': 818, 'them,': 819, 'two': 820, 'difficulties': 821, 'addressed': 822, 'such': 823, 'help.': 824, 'sizes': 825, 'recomended': 826, 'size': 827, 'chart': 828, 'real:': 829, 'smaller': 830, 'than': 831, 'chart.': 832, 'sheer': 833, 'it!.': 834, 'item': 835, 'internet..it': 836, 'better': 837, 'store': 838, 'check': 839, 'mens': 840, 'ultrasheer:': 841, 'model': 842, 'may': 843, 'ok': 844, 'sedentary': 845, 'types,': 846, 'active': 847, 'around': 848, 'alot': 849, 'job': 850, 'consistently': 851, 'rolled': 852, 'ankles!': 853, 'Good!!': 854, 'Solution:': 855, 'standard': 856, 'stocking,': 857, '20-30,': 858, 'stock': 859, '#114622.': 860, 'support,': 861, 'stays': 862, 'gives': 863, 'need.': 864, 'Both': 865, 'pair': 866, 'tore': 867, 'struggled': 868, 'pull': 869, 'Good': 870, 'riddance/bad': 871, 'investment!': 872, 'Delicious': 873, 'cookie': 874, 'mix:': 875, 'funny': 876, 'product': 877, 'knowing': 878, 'mix.': 879, 'header': 880, 'quickly': 881, 'packaged': 882, 'cookies.': 883, 'But': 884, 'no,': 885, 'MIX': 886, 'noticed': 887, 'since': 888, 'title.This': 889, 'baking': 890, 'convenience': 891, 'dough': 892, 'wrapped': 893, 'plastic': 894, 'logs': 895, 'bit': 896, 'surprise.': 897, 'Mixing': 898, 'VERY': 899, 'messy': 900, '(it': 901, 'extremely': 902, 'sticky).': 903, 'flexibility': 904, 'ratio': 905, 'ingredients': 906, 'extra': 907, 'butter': 908, 'baked': 909, 'cookies': 910, 'chewy).': 911, 'really': 912, 'large': 913, 'chocolate': 914, 'chips': 915, 'it--I': 916, 'that.I': 917, 'addition': 918, \"'natural\": 919, \"flavors'\": 920, 'Another': 921, 'Abysmal': 922, 'Digital': 923, 'Copy:': 924, 'Rather': 925, 'scratches': 926, 'insect': 927, 'droppings,': 928, 'random': 929, 'pixelations': 930, 'combined': 931, 'muddy': 932, 'light': 933, 'vague': 934, 'image': 935, 'resolution.': 936, 'Probably': 937, 'cue': 938, 'packaging': 939, 'straight': 940, 'street': 941, 'corner': 942, 'bootleg': 943, 'dealer.If': 944, 'seen': 945, 'reasonably': 946, 'condition': 947, 'film': 948, 'copy,': 949, 'defining': 950, 'visuals': 951, 'crystal': 952, 'clear': 953, 'lighting': 954, 'contrasts': 955, 'black': 956, 'white.': 957, 'surrounding': 958, 'countryside': 959, \"'old\": 960, \"home'\": 961, 'scenes': 962, 'set': 963, 'early': 964, 'morning': 965, 'ground': 966, 'mists': 967, 'haze': 968, 'memory': 969, 'while': 970, 'events': 971, 'bridge': 972, 'water': 973, 'bright,': 974, 'clear,': 975, 'immediate.Here': 976, 'dull,': 977, 'dark,': 978, 'clouded.': 979, 'Or,': 980, 'remember': 981, 'timbre': 982, 'enunciation': 983, \"Captain's\": 984, 'commands,': 985, 'visuals.After': 986, 'that,': 987, 'hard': 988, 'award': 989, 'winning,': 990, 'critically': 991, 'acclaimed': 992, \"film's\": 993, 'presentation': 994, 'YOUTUBE.': 995, 'Somewhere': 996, '\"out': 997, 'there\"': 998, 'DVD': 999, 'comes': 1000, '16mm': 1001, 'public': 1002, 'library': 1003, 'reel.Just': 1004, 'none': 1005, 'appear': 1006, 'Amazon.': 1007, 'fascinating': 1008, 'insight': 1009, 'life': 1010, 'modern': 1011, 'Japanese': 1012, 'teens:': 1013, 'thoroughly': 1014, 'enjoyed': 1015, 'Rising': 1016, 'Sons': 1017, 'Daughters.': 1018, 'other': 1019, 'looks': 1020, 'society': 1021, 'point': 1022, 'view': 1023, 'people': 1024, 'poised': 1025, 'between': 1026, \"parents'\": 1027, 'age-old': 1028, 'culture': 1029, 'restraint': 1030, 'obedience': 1031, 'community,': 1032, \"peers'\": 1033, 'adulation': 1034, 'Western': 1035, 'culture.': 1036, 'True': 1037, 'form,': 1038, '\"New': 1039, 'Young\"': 1040, 'Japan': 1041, 'seem': 1042, 'creating': 1043, '\"international\"': 1044, 'blend,': 1045, 'Ando': 1046, 'demonstrates': 1047, 'vignettes': 1048, 'private': 1049, 'lives': 1050, 'members': 1051, 'family.': 1052, 'Steven': 1053, 'Wardell': 1054, 'clearly': 1055, 'talented': 1056, 'author,': 1057, 'adopted': 1058, 'schooling': 1059, 'four': 1060, 'teens,': 1061, 'thus': 1062, 'able': 1063, 'inside': 1064, 'out.': 1065, 'read!': 1066, 'i': 1067, 'liked': 1068, 'album': 1069, 'would:': 1070, 'o': 1071, 'o,but': 1072, 'listened': 1073, '\"blue': 1074, 'angel\",\"lanna\"': 1075, '\\'mama\"': 1076, 'hair': 1077, 'rose': 1078, 'neck.Roy': 1079, 'trully': 1080, 'singer': 1081, 'talent': 1082, 'find': 1083, 'days.': 1084, 'Problem': 1085, 'charging': 1086, 'AAAs:': 1087, 'charger': 1088, 'charges': 1089, 'AA': 1090, 'batteries': 1091, 'fine,': 1092, 'huge': 1093, 'problem': 1094, 'securing': 1095, 'AAA': 1096, 'batteries.': 1097, 'To': 1098, 'charge': 1099, 'need': 1100, 'flip': 1101, 'little': 1102, 'button': 1103, 'positive': 1104, 'end.': 1105, 'In': 1106, 'pop': 1107, 'up,': 1108, \"won't\": 1109, 'hold.': 1110, 'mechanism': 1111, 'became': 1112, 'loose,': 1113, 'horizontal': 1114, 'pressure': 1115, 'push': 1116, 'buttons': 1117, 'up.': 1118, 'What': 1119, 'do': 1120, 'using': 1121, 'duct': 1122, 'tape': 1123, 'segment': 1124, 'crayon,': 1125, 'apply': 1126, 'crayon': 1127, 'buttons,': 1128, 'wrap': 1129, 'around.': 1130, 'You': 1131, 'painful': 1132, 'is.': 1133, 'Works,': 1134, 'advertised:': 1135, 'chargers..the': 1136, 'instructions': 1137, 'lights': 1138, 'stay': 1139, 'battery': 1140, 'charges...true.': 1141, 'doNT': 1142, 'turn': 1143, 'done.': 1144, 'Which': 1145, '24': 1146, 'hours': 1147, 'returned': 1148, 'thinking': 1149, 'unit.The': 1150, 'new': 1151, 'kept': 1152, 'does': 1153, 'charge...but': 1154, 'useless': 1155, '\"backup\"': 1156, 'manage': 1157, 'drain': 1158, 'AAs': 1159, \"wouldn't\": 1160, 'charger.': 1161, 'Disappointed:': 1162, 'reviews,made': 1163, 'purchase': 1164, 'disappointed.': 1165, 'convenient': 1166, 'once': 1167, 'lasts': 1168, 'short': 1169, 'longer': 1170, 'kodak': 1171, 'NiMH': 1172, 'dear:': 1173, 'excited': 1174, 'ostensibly': 1175, 'Muslim': 1176, 'feminism,': 1177, 'volume': 1178, 'live': 1179, 'expectations.One': 1180, 'essay,': 1181, 'among': 1182, 'things,': 1183, 'describes': 1184, 'veil': 1185, 'potentially': 1186, 'liberating.': 1187, \"doesn't\": 1188, 'explain': 1189, 'why.Another,': 1190, 'women': 1191, 'Cape': 1192, 'Town,': 1193, 'claims': 1194, 'separate': 1195, '\"more': 1196, 'equal.\"': 1197, 'Gee': 1198, 'whiz,': 1199, 'disappointment.I': 1200, 'hoped': 1201, 'feminist': 1202, 'condemnation': 1203, 'gender': 1204, 'apartheid.': 1205, \"book.I'm\": 1206, 'essay': 1207, 'extolling': 1208, 'virtues': 1209, 'female': 1210, 'genital': 1211, 'mutilation.--Alyssa': 1212, 'A.': 1213, 'Lappen': 1214, 'Based': 1215, 'did!:': 1216, 'VCR/DVD': 1217, 'Christmas': 1218, 'present': 1219, 'myself': 1220, 'deciding': 1221, 'join': 1222, 'rest': 1223, 'DVD-land': 1224, 'VHS': 1225, 'movies': 1226, 'yet.': 1227, 'price,': 1228, 'own': 1229, 'JVC': 1230, 'TV,': 1231, 'choice.': 1232, 'agree': 1233, 'set-up.': 1234, 'awkward': 1235, 'TV/VHS/DVD': 1236, 'selection': 1237, 'options': 1238, 'hang': 1239, 'it.Two': 1240, 'comments:': 1241, 'intuitive': 1242, 'complicated': 1243, '(too': 1244, 'many': 1245, 'remote': 1246, 'please': 1247, 'technically-minded': 1248, 'me)': 1249, 'rely': 1250, 'heavily': 1251, 'how-to': 1252, 'manual.': 1253, 'setting': 1254, 'VCR': 1255, 'timer': 1256, 'enter': 1257, 'start': 1258, 'scroll': 1259, '(unless': 1260, 'something)...but': 1261, 'complaints.': 1262, '$$$.': 1263, 'Incorrect': 1264, 'disc!:': 1265, 'big': 1266, 'fan,': 1267, 'model,': 1268, 'suspiscious': 1269, 'saw': 1270, 'units': 1271, 'return': 1272, 'section': 1273, 'store.': 1274, 'anyway': 1275, '(new)': 1276, 'happy.': 1277, 'unit': 1278, 'sends': 1279, 'clicks': 1280, 'receiver': 1281, 'while,': 1282, 'transition': 1283, 'smooth,(like': 1284, 'pause)': 1285, 'fairly': 1286, 'DVD,CD': 1287, 'headcleaner': 1288, 'work.': 1289, '\"incorrect': 1290, 'disc\"': 1291, 'message.': 1292, 'happy': 1293, 'it...but:': 1294, 'nut...I': 1295, 'televisions,': 1296, 'VCR,': 1297, 'bookshelf': 1298, 'audio': 1299, 'system': 1300, 'car': 1301, 'system.': 1302, 'So': 1303, 'came': 1304, 'move': 1305, 'player': 1306, 'boys': 1307, 'room': 1308, 'old': 1309, 'man': 1310, 'knew': 1311, 'combo': 1312, 'longer.': 1313, 'except': 1314, '2': 1315, 'things:(1)no': 1316, 'cable': 1317, 'box': 1318, 'compatability': 1319, 'control': 1320, '(2)no': 1321, 'seperate': 1322, 'inputs': 1323, 'input': 1324, 'coax': 1325, 'programming': 1326, 'mono...wife': 1327, 'tell': 1328, 'difference': 1329, \"she's\": 1330, \"I'll\": 1331, 'look': 1332, 'GREAT!!!!!!!': 1333, 'titled': 1334, '\"Hollywood': 1335, 'Debacle\":': 1336, 'ridiculous,': 1337, 'wonder': 1338, 'script': 1339, 'before': 1340, 'making': 1341, 'film.': 1342, 'mountain': 1343, 'lion': 1344, 'breaks': 1345, 'trailer': 1346, 'cars': 1347, 'behind': 1348, 'notice?': 1349, 'captured': 1350, 'jail': 1351, 'cell?': 1352, 'Get': 1353, 'real!': 1354, 'Utterly,': 1355, 'completely': 1356, 'stupid.': 1357, 'Is': 1358, 'TV???': 1359, 'bet': 1360, 'is:': 1361, 'Hotel': 1362, 'Babylon': 1363, \"TV...it's\": 1364, 'TV!!!!': 1365, 'show': 1366, 'features': 1367, 'incredible': 1368, 'acting': 1369, 'Tamzin': 1370, 'Outhwaite': 1371, '(formerly': 1372, 'EastEnders,': 1373, 'BBC': 1374, 'soap)': 1375, 'Max': 1376, 'Beesley': 1377, '(from': 1378, 'ill-fated': 1379, 'movie': 1380, '\"Glitter\"': 1381, 'starring': 1382, 'Mariah': 1383, 'Carey).': 1384, 'drama': 1385, 'series,': 1386, 'drama,': 1387, 'comedy,': 1388, 'soap': 1389, 'opera': 1390, 'mixed': 1391, 'show.': 1392, 'aired': 1393, 'America': 1394, 'seeing': 1395, 'got': 1396, 'episodes': 1397, 'great.': 1398, 'season': 1399, 'finale': 1400, 'interesting': 1401, 'watch.The': 1402, 'reminds': 1403, 'ABC': 1404, '1983': 1405, '1988.': 1406, 'reason...Hotel': 1407, 'fictional': 1408, 'San': 1409, 'Francisco': 1410, 'hotel': 1411, 'luxury': 1412, 'five-star': 1413, 'England.I': 1414, 'recommend': 1415, 'willing': 1416, 'watch': 1417, 'BBC.': 1418, 'Nothing': 1419, 'already': 1420, 'know:': 1421, 'casually': 1422, 'applying': 1423, 'law': 1424, 'school,': 1425, 'seriously!!': 1426, 'Unfortunately': 1427, \"wasn't\": 1428, 'entertaining': 1429, 'bit.:': 1430, 'ordered': 1431, 'CD,': 1432, 'hip,': 1433, 'daddy': 1434, 'vibe': 1435, 'CD.': 1436, 'However': 1437, 'dismay': 1438, 'sounds': 1439, 'fourth': 1440, 'class.': 1441, 'main': 1442, 'jist': 1443, 'CD': 1444, 'xylaphone': 1445, 'playing': 1446, 'over': 1447, 'peoples': 1448, 'voices': 1449, 'trying': 1450, 'replicate': 1451, 'happening': 1452, 'party.': 1453, 'party': 1454, 'anywhere': 1455, 'neighborhood': 1456, 'laughed': 1457, 'beach.': 1458, 'Growing': 1459, 'surfer': 1460, 'Diego,': 1461, 'Southern': 1462, 'California': 1463, 'brothers.': 1464, 'Honestly,': 1465, 'kinda': 1466, 'B': 1467, 'movie.': 1468, 'absolutle': 1469, 'epitimy': 1470, 'last': 1471, '\"vibe\"': 1472, 'Surf': 1473, 'Cha': 1474, 'Cha.': 1475, 'Surfers': 1476, 'CHA.': 1477, 'Rochelle': 1478, 'explains': 1479, 'You:': 1480, 'Wondering': 1481, 'hell': 1482, 'happened': 1483, 'moral': 1484, 'aspect': 1485, 'American': 1486, '?': 1487, 'lucid,': 1488, 'well': 1489, 'argued': 1490, 'explanation': 1491, 'simple': 1492, 'become': 1493, 'focused': 1494, 'our': 1495, 'individual': 1496, 'RIGHTS': 1497, 'ignored,': 1498, 'mocked,': 1499, 'personal': 1500, 'responsibilities.': 1501, 'final': 1502, 'response': 1503, 'indictment': 1504, 'Robert': 1505, \"Ringer's\": 1506, 'seller,': 1507, 'LOOKING': 1508, 'OUT': 1509, 'FOR': 1510, '#1.': 1511, 'disgusted': 1512, 'boorish': 1513, 'state': 1514, 'media,': 1515, 'politics': 1516, 'discourse': 1517, 'general,': 1518, 'heads': 1519, 'substantial': 1520, 'challenges': 1521, 'lie': 1522, 'Americans,': 1523, 'human': 1524, 'beings.': 1525}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpqEFoNMGDno",
        "colab_type": "text"
      },
      "source": [
        " provide one-hot coding with hashing (listing 6.4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uPkv0gVGN4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "samples = texts[:36] # using only first 36 reviews"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Vqlli-gFpCn",
        "colab_type": "code",
        "outputId": "e9c61eae-6114-4d9d-a56b-dca7efbef825",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "dimensionality = 1000\n",
        "max_length = 10\n",
        "results = np.zeros((len(samples), max_length, dimensionality))\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "    index = abs(hash(word)) % dimensionality\n",
        "    results[i, j, index] = 1.\n",
        "\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1229 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL1Ejw-LGKir",
        "colab_type": "code",
        "outputId": "bdf45ae4-dd5e-4a8c-901c-e5c9c7df88f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(token_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'The': 1, 'best': 2, 'soundtrack': 3, 'ever': 4, 'to': 5, 'anything.:': 6, \"I'm\": 7, 'reading': 8, 'a': 9, 'lot': 10, 'of': 11, 'reviews': 12, 'saying': 13, 'that': 14, 'this': 15, 'is': 16, 'the': 17, \"'game\": 18, \"soundtrack'\": 19, 'and': 20, 'I': 21, 'figured': 22, \"I'd\": 23, 'write': 24, 'review': 25, 'disagree': 26, 'bit.': 27, 'This': 28, 'in': 29, 'my': 30, 'opinino': 31, 'Yasunori': 32, \"Mitsuda's\": 33, 'ultimate': 34, 'masterpiece.': 35, 'music': 36, 'timeless': 37, 'been': 38, 'listening': 39, 'it': 40, 'for': 41, 'years': 42, 'now': 43, 'its': 44, 'beauty': 45, 'simply': 46, 'refuses': 47, 'fade.The': 48, 'price': 49, 'tag': 50, 'on': 51, 'pretty': 52, 'staggering': 53, 'must': 54, 'say,': 55, 'but': 56, 'if': 57, 'you': 58, 'are': 59, 'going': 60, 'buy': 61, 'any': 62, 'cd': 63, 'much': 64, 'money,': 65, 'only': 66, 'one': 67, 'feel': 68, 'would': 69, 'be': 70, 'worth': 71, 'every': 72, 'penny.': 73, 'Amazing!:': 74, 'favorite': 75, 'all': 76, 'time,': 77, 'hands': 78, 'down.': 79, 'intense': 80, 'sadness': 81, '\"Prisoners': 82, 'Fate\"': 83, '(which': 84, 'means': 85, 'more': 86, \"you've\": 87, 'played': 88, 'game)': 89, 'hope': 90, '\"A': 91, 'Distant': 92, 'Promise\"': 93, '\"Girl': 94, 'who': 95, 'Stole': 96, 'Star\"': 97, 'have': 98, 'an': 99, 'important': 100, 'inspiration': 101, 'me': 102, 'personally': 103, 'throughout': 104, 'teen': 105, 'years.': 106, 'higher': 107, 'energy': 108, 'tracks': 109, 'like': 110, '\"Chrono': 111, 'Cross': 112, '~': 113, \"Time's\": 114, 'Scar~\",': 115, '\"Time': 116, 'Dreamwatch\",': 117, '\"Chronomantique\"': 118, '(indefinably': 119, 'remeniscent': 120, 'Chrono': 121, 'Trigger)': 122, 'absolutely': 123, 'superb': 124, 'as': 125, 'well.This': 126, 'amazing': 127, 'music,': 128, 'probably': 129, \"composer's\": 130, 'work': 131, '(I': 132, \"haven't\": 133, 'heard': 134, 'Xenogears': 135, 'soundtrack,': 136, 'so': 137, \"can't\": 138, 'say': 139, 'sure),': 140, 'even': 141, 'never': 142, 'game,': 143, 'twice': 144, 'it.I': 145, 'wish': 146, 'could': 147, 'give': 148, '6': 149, 'stars.': 150, 'Excellent': 151, 'Soundtrack:': 152, 'truly': 153, 'enjoy': 154, 'video': 155, 'game': 156, 'music.': 157, 'most': 158, 'here': 159, \"it's\": 160, 'relaxing': 161, 'peaceful.On': 162, 'disk': 163, 'one.': 164, 'favorites': 165, 'Scars': 166, 'Of': 167, 'Time,': 168, 'Between': 169, 'Life': 170, 'Death,': 171, 'Forest': 172, 'Illusion,': 173, 'Fortress': 174, 'Ancient': 175, 'Dragons,': 176, 'Lost': 177, 'Fragment,': 178, 'Drowned': 179, 'Valley.Disk': 180, 'Two:': 181, 'Draggons,': 182, 'Galdorb': 183, '-': 184, 'Home,': 185, 'Chronomantique,': 186, 'Prisoners': 187, 'Fate,': 188, 'Gale,': 189, 'girlfriend': 190, 'likes': 191, 'ZelbessDisk': 192, 'Three:': 193, 'three.': 194, 'Garden': 195, 'God,': 196, 'Chronopolis,': 197, 'Fates,': 198, 'Jellyfish': 199, 'sea,': 200, 'Burning': 201, 'Orphange,': 202, \"Dragon's\": 203, 'Prayer,': 204, 'Tower': 205, 'Stars,': 206, 'Dragon': 207, 'Radical': 208, 'Dreamers': 209, 'Unstealable': 210, 'Jewel.Overall,': 211, 'excellent': 212, 'should': 213, 'brought': 214, 'by': 215, 'those': 216, 'music.Xander': 217, 'Remember,': 218, 'Pull': 219, 'Your': 220, 'Jaw': 221, 'Off': 222, 'Floor': 223, 'After': 224, 'Hearing': 225, 'it:': 226, 'If': 227, 'know': 228, 'how': 229, 'divine': 230, 'is!': 231, 'Every': 232, 'single': 233, 'song': 234, 'tells': 235, 'story': 236, 'good!': 237, 'greatest': 238, 'songs': 239, 'without': 240, 'doubt,': 241, 'Cross:': 242, 'Scar,': 243, 'Magical': 244, 'Dreamers:': 245, 'Wind,': 246, 'Sea': 247, 'Unstolen': 248, 'Jewel.': 249, '(Translation': 250, 'varies)': 251, 'perfect': 252, 'ask': 253, 'me,': 254, 'can': 255, 'be.': 256, 'Mitsuda': 257, 'just': 258, 'poured': 259, 'his': 260, 'heart': 261, 'wrote': 262, 'down': 263, 'paper.': 264, 'absolute': 265, 'masterpiece:': 266, 'am': 267, 'quite': 268, 'sure': 269, 'actually': 270, 'taking': 271, 'time': 272, 'read': 273, 'at': 274, 'least': 275, 'once,': 276, 'few': 277, 'here.': 278, 'And': 279, 'whether': 280, 'were': 281, 'aware': 282, 'or': 283, 'not,': 284, 'contributed': 285, 'greatly': 286, 'mood': 287, 'minute': 288, 'whole': 289, 'game.Composed': 290, '3': 291, 'CDs': 292, 'exact': 293, 'count),': 294, 'which': 295, 'heart-rendering': 296, 'impressively': 297, 'remarkable,': 298, 'assure': 299, 'will': 300, 'not': 301, 'forget.': 302, 'It': 303, 'has': 304, 'everything': 305, 'listener': 306, '--': 307, 'from': 308, 'fast-paced': 309, 'energetic': 310, '(Dancing': 311, 'Tokage': 312, 'Termina': 313, 'Home),': 314, 'slower': 315, 'haunting': 316, '(Dragon': 317, 'God),': 318, 'purely': 319, 'beautifully': 320, 'composed': 321, \"(Time's\": 322, 'Scar),': 323, 'some': 324, 'fantastic': 325, 'vocals': 326, '(Radical': 327, 'Dreamers).This': 328, 'videogame': 329, 'soundtracks': 330, 'out': 331, 'there,': 332, 'surely': 333, 'ever.': 334, '^_^': 335, 'Buyer': 336, 'beware:': 337, 'self-published': 338, 'book,': 339, 'want': 340, 'why--read': 341, 'paragraphs!': 342, 'Those': 343, '5': 344, 'star': 345, 'written': 346, 'Ms.': 347, \"Haddon's\": 348, 'family': 349, 'friends--or': 350, 'perhaps,': 351, 'herself!': 352, 'imagine': 353, 'anyone': 354, 'thing--I': 355, 'spent': 356, 'evening': 357, 'with': 358, 'book': 359, 'friend': 360, 'we': 361, 'hysterics': 362, 'bits': 363, 'pieces': 364, 'another.': 365, 'definitely': 366, 'bad': 367, 'enough': 368, 'entered': 369, 'into': 370, 'kind': 371, '\"worst': 372, 'book\"': 373, 'contest.': 374, 'believe': 375, 'Amazon': 376, 'sells': 377, 'thing.': 378, 'Maybe': 379, 'offer': 380, 'them': 381, '8th': 382, 'grade': 383, 'term': 384, 'paper': 385, '\"To': 386, 'Kill': 387, 'Mockingbird\"--a': 388, 'Haddon': 389, 'of.': 390, 'Anyway,': 391, 'unless': 392, 'send': 393, 'someone': 394, 'joke---stay': 395, 'far,': 396, 'far': 397, 'away': 398, 'one!': 399, 'Glorious': 400, 'story:': 401, 'loved': 402, 'Whisper': 403, 'wicked': 404, 'saints.': 405, 'was': 406, 'pleasantly': 407, 'surprised': 408, 'changes': 409, 'book.': 410, 'normaly': 411, 'romance': 412, 'novels,': 413, 'world': 414, 'raving': 415, 'about': 416, 'bought': 417, 'it.': 418, '!!': 419, 'brilliant': 420, 'because': 421, 'true.': 422, 'wonderful': 423, 'told': 424, 'friends': 425, 'typical': 426, 'romance,': 427, 'more.': 428, 'Not': 429, 'crime,': 430, 'becuase': 431, 'missing': 432, 'warming': 433, 'story.': 434, 'A': 435, 'FIVE': 436, 'STAR': 437, 'BOOK:': 438, 'finished': 439, 'Wicked': 440, 'fell': 441, 'love': 442, 'caracters.': 443, 'expected': 444, 'average': 445, 'read,': 446, 'instead': 447, 'found': 448, 'books': 449, 'time.': 450, 'Just': 451, 'when': 452, 'thought': 453, 'predict': 454, 'outcome': 455, 'shocked': 456, '!': 457, 'writting': 458, 'descriptive': 459, 'broke': 460, \"Julia's\": 461, 'did': 462, 'felt': 463, 'there': 464, 'distant': 465, 'reader.': 466, 'lover': 467, 'novels': 468, 'then': 469, 'read.': 470, \"Don't\": 471, 'let': 472, 'cover': 473, 'fool': 474, 'spectacular!': 475, 'Whispers': 476, 'Saints:': 477, 'easy': 478, 'made': 479, 'keep': 480, 'on,': 481, 'put': 482, 'down.It': 483, 'left': 484, 'wanting': 485, 'follow': 486, 'coming': 487, 'soon.': 488, 'used': 489, 'gotten': 490, 'again.': 491, 'Very': 492, 'enjoyable.': 493, 'Worst!:': 494, 'complete': 495, 'waste': 496, 'Typographical': 497, 'errors,': 498, 'poor': 499, 'grammar,': 500, 'totally': 501, 'pathetic': 502, 'plot': 503, 'add': 504, 'up': 505, 'nothing.': 506, 'embarrassed': 507, 'author': 508, 'very': 509, 'disappointed': 510, 'paid': 511, 'Great': 512, 'book:': 513, 'great': 514, 'book,I': 515, 'down,and': 516, 'fast': 517, 'enough.': 518, 'Boy': 519, 'what': 520, 'twist': 521, 'turns': 522, 'keeps': 523, 'guessing': 524, 'happen': 525, 'next.': 526, 'makes': 527, 'fall': 528, 'heat': 529, 'up,it': 530, 'also': 531, 'make': 532, 'angery.': 533, 'go': 534, 'throu': 535, 'several': 536, 'your': 537, 'emotions.': 538, 'quick': 539, 'romance.': 540, 'something': 541, 'end': 542, 'day': 543, 'off': 544, 'night.': 545, 'Read:': 546, 'brilliant,': 547, 'yet': 548, 'realistic.': 549, 'showed': 550, 'error': 551, 'human.': 552, 'fact': 553, 'writer': 554, 'loving': 555, 'side': 556, 'God': 557, 'revengeful': 558, 'him.': 559, 'twisted': 560, 'turned': 561, 'glass': 562, 'castle.': 563, 'Oh': 564, 'please:': 565, 'guess': 566, 'novel': 567, 'one,': 568, 'discerning': 569, 'All': 570, 'others': 571, 'beware!': 572, 'drivel.': 573, 'trouble': 574, 'typo': 575, 'prominently': 576, 'featured': 577, 'back': 578, 'cover,': 579, 'first': 580, 'page': 581, 'removed': 582, 'doubt.': 583, 'Wait': 584, 'maybe': 585, 'point.': 586, 're-read': 587, 'beginning': 588, 'clear.': 589, 'intentional': 590, 'churning': 591, 'over-heated': 592, 'prose': 593, 'satiric': 594, 'purposes.': 595, 'Phew,': 596, 'glad': 597, \"didn't\": 598, '$10.95': 599, 'after': 600, 'all.': 601, 'Awful': 602, 'beyond': 603, 'belief!:': 604, 'wasting': 605, 'their': 606, 'money.': 607, 'seems': 608, '7th': 609, 'grader': 610, 'grammatical': 611, 'skills': 612, 'her': 613, 'age!': 614, 'As': 615, 'another': 616, 'reviewer': 617, 'points': 618, 'out,': 619, 'misspelling': 620, 'per': 621, 'chapter.': 622, 'For': 623, 'example,': 624, 'mentioned': 625, 'she': 626, 'had': 627, '\"lean\"': 628, 'house.': 629, 'distracted': 630, 'writing': 631, 'weak': 632, 'plot,': 633, 'decided': 634, 'pencil': 635, 'hand': 636, 'mark': 637, 'horrible': 638, 'grammar': 639, 'spelling.': 640, 'Please': 641, \"don't\": 642, 'too,': 643, 'good': 644, \"author's\": 645, 'relatives.': 646, 'faith': 647, 'on!': 648, 'try': 649, 'us': 650, 'fake': 651, 'reviews.:': 652, \"It's\": 653, 'glaringly': 654, 'obvious': 655, 'glowing': 656, 'same': 657, 'person,': 658, 'perhaps': 659, 'herself.': 660, 'They': 661, 'misspellings': 662, 'sentence': 663, 'structure': 664, 'Who': 665, 'Veronica': 666, 'think': 667, 'author?': 668, 'romantic': 669, 'zen': 670, 'baseball': 671, 'comedy:': 672, 'When': 673, 'hear': 674, 'folks': 675, 'they': 676, \"'em\": 677, 'anymore,': 678, 'might': 679, 'talking': 680, '\"BY': 681, 'THE': 682, 'SEA\".': 683, 'cool': 684, 'young': 685, 'Cuban': 686, 'girl': 687, 'searching': 688, 'idenity': 689, 'stumbles': 690, 'coastal': 691, 'resort': 692, 'kitchen': 693, 'gig': 694, 'motorcycle': 695, 'maintenance': 696, 'man,': 697, 'three': 698, 'hysterical': 699, 'Italian': 700, 'chefs': 701, 'Latino': 702, 'fireballing': 703, 'right': 704, 'handed': 705, 'pitcher': 706, 'plays': 707, 'team': 708, 'sponsored': 709, \"resort's\": 710, 'owner.': 711, 'often': 712, 'case': 713, \"'finds'\": 714, 'herself': 715, 'through': 716, 'honest,': 717, 'comical': 718, 'always': 719, 'emotional,': 720, 'interaction': 721, 'sizzling': 722, 'roster': 723, 'players.': 724, 'With': 725, 'mix': 726, 'special': 727, 'effects,': 728, 'salsa': 729, 'sound': 730, 'flashbacks,': 731, 'BY': 732, 'SEA,': 733, 'gets': 734, '4': 735, 'BIG': 736, 'stars': 737, 'me!': 738, 'Fashionable': 739, 'Compression': 740, 'Stockings!:': 741, 'DVT': 742, 'doctor': 743, 'required': 744, 'wear': 745, 'compression': 746, 'stockings.': 747, 'wore': 748, 'ugly': 749, 'white': 750, 'TED': 751, 'hose': 752, 'yucky': 753, 'thick': 754, 'brown': 755, 'Then': 756, 'Jobst': 757, 'UltraSheer.': 758, 'gave': 759, 'needed': 760, '(15-20,)': 761, 'looked': 762, 'regular': 763, 'pantyhose.': 764, 'Even': 765, 'though': 766, 'blood': 767, 'clot': 768, 'gone': 769, 'years,': 770, 'still': 771, 'these': 772, 'support': 773, 'stockings': 774, 'legs': 775, 'nice.**Note,': 776, 'problems': 777, 'rubberized': 778, 'tops': 779, 'rolling': 780, 'thigh.': 781, 'tried': 782, 'adhesive,': 783, 'hated': 784, 'having': 785, 'skin': 786, 'pulled': 787, 'day.': 788, 'inexpensive': 789, 'garter': 790, 'belt': 791, 'works': 792, 'fine': 793, 'helps': 794, 'rolling.': 795, 'UltraSheer': 796, 'Thigh': 797, 'High:': 798, 'product.': 799, 'However,': 800, 'difficult': 801, 'get': 802, 'older': 803, 'people.': 804, \"I've\": 805, 'full': 806, 'workout': 807, 'getting': 808, 'on.': 809, 'Also,': 810, 'wears': 811, 'begin': 812, 'roll': 813, 'top': 814, 'create': 815, 'deep': 816, 'ridge': 817, 'skin.': 818, 'them,': 819, 'two': 820, 'difficulties': 821, 'addressed': 822, 'such': 823, 'help.': 824, 'sizes': 825, 'recomended': 826, 'size': 827, 'chart': 828, 'real:': 829, 'smaller': 830, 'than': 831, 'chart.': 832, 'sheer': 833, 'it!.': 834, 'item': 835, 'internet..it': 836, 'better': 837, 'store': 838, 'check': 839, 'mens': 840, 'ultrasheer:': 841, 'model': 842, 'may': 843, 'ok': 844, 'sedentary': 845, 'types,': 846, 'active': 847, 'around': 848, 'alot': 849, 'job': 850, 'consistently': 851, 'rolled': 852, 'ankles!': 853, 'Good!!': 854, 'Solution:': 855, 'standard': 856, 'stocking,': 857, '20-30,': 858, 'stock': 859, '#114622.': 860, 'support,': 861, 'stays': 862, 'gives': 863, 'need.': 864, 'Both': 865, 'pair': 866, 'tore': 867, 'struggled': 868, 'pull': 869, 'Good': 870, 'riddance/bad': 871, 'investment!': 872, 'Delicious': 873, 'cookie': 874, 'mix:': 875, 'funny': 876, 'product': 877, 'knowing': 878, 'mix.': 879, 'header': 880, 'quickly': 881, 'packaged': 882, 'cookies.': 883, 'But': 884, 'no,': 885, 'MIX': 886, 'noticed': 887, 'since': 888, 'title.This': 889, 'baking': 890, 'convenience': 891, 'dough': 892, 'wrapped': 893, 'plastic': 894, 'logs': 895, 'bit': 896, 'surprise.': 897, 'Mixing': 898, 'VERY': 899, 'messy': 900, '(it': 901, 'extremely': 902, 'sticky).': 903, 'flexibility': 904, 'ratio': 905, 'ingredients': 906, 'extra': 907, 'butter': 908, 'baked': 909, 'cookies': 910, 'chewy).': 911, 'really': 912, 'large': 913, 'chocolate': 914, 'chips': 915, 'it--I': 916, 'that.I': 917, 'addition': 918, \"'natural\": 919, \"flavors'\": 920, 'Another': 921, 'Abysmal': 922, 'Digital': 923, 'Copy:': 924, 'Rather': 925, 'scratches': 926, 'insect': 927, 'droppings,': 928, 'random': 929, 'pixelations': 930, 'combined': 931, 'muddy': 932, 'light': 933, 'vague': 934, 'image': 935, 'resolution.': 936, 'Probably': 937, 'cue': 938, 'packaging': 939, 'straight': 940, 'street': 941, 'corner': 942, 'bootleg': 943, 'dealer.If': 944, 'seen': 945, 'reasonably': 946, 'condition': 947, 'film': 948, 'copy,': 949, 'defining': 950, 'visuals': 951, 'crystal': 952, 'clear': 953, 'lighting': 954, 'contrasts': 955, 'black': 956, 'white.': 957, 'surrounding': 958, 'countryside': 959, \"'old\": 960, \"home'\": 961, 'scenes': 962, 'set': 963, 'early': 964, 'morning': 965, 'ground': 966, 'mists': 967, 'haze': 968, 'memory': 969, 'while': 970, 'events': 971, 'bridge': 972, 'water': 973, 'bright,': 974, 'clear,': 975, 'immediate.Here': 976, 'dull,': 977, 'dark,': 978, 'clouded.': 979, 'Or,': 980, 'remember': 981, 'timbre': 982, 'enunciation': 983, \"Captain's\": 984, 'commands,': 985, 'visuals.After': 986, 'that,': 987, 'hard': 988, 'award': 989, 'winning,': 990, 'critically': 991, 'acclaimed': 992, \"film's\": 993, 'presentation': 994, 'YOUTUBE.': 995, 'Somewhere': 996, '\"out': 997, 'there\"': 998, 'DVD': 999, 'comes': 1000, '16mm': 1001, 'public': 1002, 'library': 1003, 'reel.Just': 1004, 'none': 1005, 'appear': 1006, 'Amazon.': 1007, 'fascinating': 1008, 'insight': 1009, 'life': 1010, 'modern': 1011, 'Japanese': 1012, 'teens:': 1013, 'thoroughly': 1014, 'enjoyed': 1015, 'Rising': 1016, 'Sons': 1017, 'Daughters.': 1018, 'other': 1019, 'looks': 1020, 'society': 1021, 'point': 1022, 'view': 1023, 'people': 1024, 'poised': 1025, 'between': 1026, \"parents'\": 1027, 'age-old': 1028, 'culture': 1029, 'restraint': 1030, 'obedience': 1031, 'community,': 1032, \"peers'\": 1033, 'adulation': 1034, 'Western': 1035, 'culture.': 1036, 'True': 1037, 'form,': 1038, '\"New': 1039, 'Young\"': 1040, 'Japan': 1041, 'seem': 1042, 'creating': 1043, '\"international\"': 1044, 'blend,': 1045, 'Ando': 1046, 'demonstrates': 1047, 'vignettes': 1048, 'private': 1049, 'lives': 1050, 'members': 1051, 'family.': 1052, 'Steven': 1053, 'Wardell': 1054, 'clearly': 1055, 'talented': 1056, 'author,': 1057, 'adopted': 1058, 'schooling': 1059, 'four': 1060, 'teens,': 1061, 'thus': 1062, 'able': 1063, 'inside': 1064, 'out.': 1065, 'read!': 1066, 'i': 1067, 'liked': 1068, 'album': 1069, 'would:': 1070, 'o': 1071, 'o,but': 1072, 'listened': 1073, '\"blue': 1074, 'angel\",\"lanna\"': 1075, '\\'mama\"': 1076, 'hair': 1077, 'rose': 1078, 'neck.Roy': 1079, 'trully': 1080, 'singer': 1081, 'talent': 1082, 'find': 1083, 'days.': 1084, 'Problem': 1085, 'charging': 1086, 'AAAs:': 1087, 'charger': 1088, 'charges': 1089, 'AA': 1090, 'batteries': 1091, 'fine,': 1092, 'huge': 1093, 'problem': 1094, 'securing': 1095, 'AAA': 1096, 'batteries.': 1097, 'To': 1098, 'charge': 1099, 'need': 1100, 'flip': 1101, 'little': 1102, 'button': 1103, 'positive': 1104, 'end.': 1105, 'In': 1106, 'pop': 1107, 'up,': 1108, \"won't\": 1109, 'hold.': 1110, 'mechanism': 1111, 'became': 1112, 'loose,': 1113, 'horizontal': 1114, 'pressure': 1115, 'push': 1116, 'buttons': 1117, 'up.': 1118, 'What': 1119, 'do': 1120, 'using': 1121, 'duct': 1122, 'tape': 1123, 'segment': 1124, 'crayon,': 1125, 'apply': 1126, 'crayon': 1127, 'buttons,': 1128, 'wrap': 1129, 'around.': 1130, 'You': 1131, 'painful': 1132, 'is.': 1133, 'Works,': 1134, 'advertised:': 1135, 'chargers..the': 1136, 'instructions': 1137, 'lights': 1138, 'stay': 1139, 'battery': 1140, 'charges...true.': 1141, 'doNT': 1142, 'turn': 1143, 'done.': 1144, 'Which': 1145, '24': 1146, 'hours': 1147, 'returned': 1148, 'thinking': 1149, 'unit.The': 1150, 'new': 1151, 'kept': 1152, 'does': 1153, 'charge...but': 1154, 'useless': 1155, '\"backup\"': 1156, 'manage': 1157, 'drain': 1158, 'AAs': 1159, \"wouldn't\": 1160, 'charger.': 1161, 'Disappointed:': 1162, 'reviews,made': 1163, 'purchase': 1164, 'disappointed.': 1165, 'convenient': 1166, 'once': 1167, 'lasts': 1168, 'short': 1169, 'longer': 1170, 'kodak': 1171, 'NiMH': 1172, 'dear:': 1173, 'excited': 1174, 'ostensibly': 1175, 'Muslim': 1176, 'feminism,': 1177, 'volume': 1178, 'live': 1179, 'expectations.One': 1180, 'essay,': 1181, 'among': 1182, 'things,': 1183, 'describes': 1184, 'veil': 1185, 'potentially': 1186, 'liberating.': 1187, \"doesn't\": 1188, 'explain': 1189, 'why.Another,': 1190, 'women': 1191, 'Cape': 1192, 'Town,': 1193, 'claims': 1194, 'separate': 1195, '\"more': 1196, 'equal.\"': 1197, 'Gee': 1198, 'whiz,': 1199, 'disappointment.I': 1200, 'hoped': 1201, 'feminist': 1202, 'condemnation': 1203, 'gender': 1204, 'apartheid.': 1205, \"book.I'm\": 1206, 'essay': 1207, 'extolling': 1208, 'virtues': 1209, 'female': 1210, 'genital': 1211, 'mutilation.--Alyssa': 1212, 'A.': 1213, 'Lappen': 1214, 'Based': 1215, 'did!:': 1216, 'VCR/DVD': 1217, 'Christmas': 1218, 'present': 1219, 'myself': 1220, 'deciding': 1221, 'join': 1222, 'rest': 1223, 'DVD-land': 1224, 'VHS': 1225, 'movies': 1226, 'yet.': 1227, 'price,': 1228, 'own': 1229, 'JVC': 1230, 'TV,': 1231, 'choice.': 1232, 'agree': 1233, 'set-up.': 1234, 'awkward': 1235, 'TV/VHS/DVD': 1236, 'selection': 1237, 'options': 1238, 'hang': 1239, 'it.Two': 1240, 'comments:': 1241, 'intuitive': 1242, 'complicated': 1243, '(too': 1244, 'many': 1245, 'remote': 1246, 'please': 1247, 'technically-minded': 1248, 'me)': 1249, 'rely': 1250, 'heavily': 1251, 'how-to': 1252, 'manual.': 1253, 'setting': 1254, 'VCR': 1255, 'timer': 1256, 'enter': 1257, 'start': 1258, 'scroll': 1259, '(unless': 1260, 'something)...but': 1261, 'complaints.': 1262, '$$$.': 1263, 'Incorrect': 1264, 'disc!:': 1265, 'big': 1266, 'fan,': 1267, 'model,': 1268, 'suspiscious': 1269, 'saw': 1270, 'units': 1271, 'return': 1272, 'section': 1273, 'store.': 1274, 'anyway': 1275, '(new)': 1276, 'happy.': 1277, 'unit': 1278, 'sends': 1279, 'clicks': 1280, 'receiver': 1281, 'while,': 1282, 'transition': 1283, 'smooth,(like': 1284, 'pause)': 1285, 'fairly': 1286, 'DVD,CD': 1287, 'headcleaner': 1288, 'work.': 1289, '\"incorrect': 1290, 'disc\"': 1291, 'message.': 1292, 'happy': 1293, 'it...but:': 1294, 'nut...I': 1295, 'televisions,': 1296, 'VCR,': 1297, 'bookshelf': 1298, 'audio': 1299, 'system': 1300, 'car': 1301, 'system.': 1302, 'So': 1303, 'came': 1304, 'move': 1305, 'player': 1306, 'boys': 1307, 'room': 1308, 'old': 1309, 'man': 1310, 'knew': 1311, 'combo': 1312, 'longer.': 1313, 'except': 1314, '2': 1315, 'things:(1)no': 1316, 'cable': 1317, 'box': 1318, 'compatability': 1319, 'control': 1320, '(2)no': 1321, 'seperate': 1322, 'inputs': 1323, 'input': 1324, 'coax': 1325, 'programming': 1326, 'mono...wife': 1327, 'tell': 1328, 'difference': 1329, \"she's\": 1330, \"I'll\": 1331, 'look': 1332, 'GREAT!!!!!!!': 1333, 'titled': 1334, '\"Hollywood': 1335, 'Debacle\":': 1336, 'ridiculous,': 1337, 'wonder': 1338, 'script': 1339, 'before': 1340, 'making': 1341, 'film.': 1342, 'mountain': 1343, 'lion': 1344, 'breaks': 1345, 'trailer': 1346, 'cars': 1347, 'behind': 1348, 'notice?': 1349, 'captured': 1350, 'jail': 1351, 'cell?': 1352, 'Get': 1353, 'real!': 1354, 'Utterly,': 1355, 'completely': 1356, 'stupid.': 1357, 'Is': 1358, 'TV???': 1359, 'bet': 1360, 'is:': 1361, 'Hotel': 1362, 'Babylon': 1363, \"TV...it's\": 1364, 'TV!!!!': 1365, 'show': 1366, 'features': 1367, 'incredible': 1368, 'acting': 1369, 'Tamzin': 1370, 'Outhwaite': 1371, '(formerly': 1372, 'EastEnders,': 1373, 'BBC': 1374, 'soap)': 1375, 'Max': 1376, 'Beesley': 1377, '(from': 1378, 'ill-fated': 1379, 'movie': 1380, '\"Glitter\"': 1381, 'starring': 1382, 'Mariah': 1383, 'Carey).': 1384, 'drama': 1385, 'series,': 1386, 'drama,': 1387, 'comedy,': 1388, 'soap': 1389, 'opera': 1390, 'mixed': 1391, 'show.': 1392, 'aired': 1393, 'America': 1394, 'seeing': 1395, 'got': 1396, 'episodes': 1397, 'great.': 1398, 'season': 1399, 'finale': 1400, 'interesting': 1401, 'watch.The': 1402, 'reminds': 1403, 'ABC': 1404, '1983': 1405, '1988.': 1406, 'reason...Hotel': 1407, 'fictional': 1408, 'San': 1409, 'Francisco': 1410, 'hotel': 1411, 'luxury': 1412, 'five-star': 1413, 'England.I': 1414, 'recommend': 1415, 'willing': 1416, 'watch': 1417, 'BBC.': 1418, 'Nothing': 1419, 'already': 1420, 'know:': 1421, 'casually': 1422, 'applying': 1423, 'law': 1424, 'school,': 1425, 'seriously!!': 1426, 'Unfortunately': 1427, \"wasn't\": 1428, 'entertaining': 1429, 'bit.:': 1430, 'ordered': 1431, 'CD,': 1432, 'hip,': 1433, 'daddy': 1434, 'vibe': 1435, 'CD.': 1436, 'However': 1437, 'dismay': 1438, 'sounds': 1439, 'fourth': 1440, 'class.': 1441, 'main': 1442, 'jist': 1443, 'CD': 1444, 'xylaphone': 1445, 'playing': 1446, 'over': 1447, 'peoples': 1448, 'voices': 1449, 'trying': 1450, 'replicate': 1451, 'happening': 1452, 'party.': 1453, 'party': 1454, 'anywhere': 1455, 'neighborhood': 1456, 'laughed': 1457, 'beach.': 1458, 'Growing': 1459, 'surfer': 1460, 'Diego,': 1461, 'Southern': 1462, 'California': 1463, 'brothers.': 1464, 'Honestly,': 1465, 'kinda': 1466, 'B': 1467, 'movie.': 1468, 'absolutle': 1469, 'epitimy': 1470, 'last': 1471, '\"vibe\"': 1472, 'Surf': 1473, 'Cha': 1474, 'Cha.': 1475, 'Surfers': 1476, 'CHA.': 1477, 'Rochelle': 1478, 'explains': 1479, 'You:': 1480, 'Wondering': 1481, 'hell': 1482, 'happened': 1483, 'moral': 1484, 'aspect': 1485, 'American': 1486, '?': 1487, 'lucid,': 1488, 'well': 1489, 'argued': 1490, 'explanation': 1491, 'simple': 1492, 'become': 1493, 'focused': 1494, 'our': 1495, 'individual': 1496, 'RIGHTS': 1497, 'ignored,': 1498, 'mocked,': 1499, 'personal': 1500, 'responsibilities.': 1501, 'final': 1502, 'response': 1503, 'indictment': 1504, 'Robert': 1505, \"Ringer's\": 1506, 'seller,': 1507, 'LOOKING': 1508, 'OUT': 1509, 'FOR': 1510, '#1.': 1511, 'disgusted': 1512, 'boorish': 1513, 'state': 1514, 'media,': 1515, 'politics': 1516, 'discourse': 1517, 'general,': 1518, 'heads': 1519, 'substantial': 1520, 'challenges': 1521, 'lie': 1522, 'Americans,': 1523, 'human': 1524, 'beings.': 1525}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jf-cQlfnGtbr",
        "colab_type": "text"
      },
      "source": [
        "4) Validate the how much they are similar. Try to maximize the similarity (ideally should be 100% the same). Specify the reason if both are not the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txY49f3vFJLo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Well the indexes they created are the same. I know first two method where same.\n",
        "# There might be some difference in hasing but i can,t see an impact on the output."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buU5uSiZIVkG",
        "colab_type": "text"
      },
      "source": [
        "5) Try to implement word-embedding using code given in listing (6.7) and shared with me the embedding array as well as the word dictionary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLh01dsXsT9L",
        "colab_type": "text"
      },
      "source": [
        "Preparing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlJUpzfPNePl",
        "colab_type": "code",
        "outputId": "4534e18b-a120-440b-f8c2-d1cd5f38e8aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import numpy as np # converting list to array\n",
        "\n",
        "texts = np.asarray(texts)\n",
        "labels = np.asarray(labels)\n",
        "\n",
        "print('Shape of texts tensor:', texts.shape)\n",
        "print('Shape of label tensor:', labels.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of texts tensor: (360000,)\n",
            "Shape of label tensor: (360000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zE6qT9QYGdhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxlen = 100               # Cuts off reviews after 100 words\n",
        "training_samples = 200     # Trains on 200 samples\n",
        "validation_samples = 10000 # Validates on 10,000 samples\n",
        "max_words = 10000          # Considers only the top 10,000 words in the dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFoO3seJGdjq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)    # max_words = 10000 \n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DvXqrhvGdmb",
        "colab_type": "code",
        "outputId": "72bbdfb9-5ba5-4ece-d721-27db0c739f11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "print()\n",
        "#print(tokenizer.word_index)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 262219 unique tokens.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuNoN7GNJG03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pad_sequences is used to ensure that all sequences in a list have the same length.\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "data = pad_sequences(sequences, maxlen=maxlen)                    # maxlen = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSMxttY5NWy_",
        "colab_type": "code",
        "outputId": "891c8541-2b6c-478d-877c-4d5a45053f5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(data,labels)\n",
        "\n",
        "print(\"x_train :\",x_train.shape)\n",
        "print(\"x_test :\",x_test.shape)\n",
        "print(\"y_train :\",y_train.shape)\n",
        "print(\"y_test :\",y_test.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train : (270000, 100)\n",
            "x_test : (90000, 100)\n",
            "y_train : (270000,)\n",
            "y_test : (90000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmjgsZz7JG3k",
        "colab_type": "code",
        "outputId": "e7177a3f-1dbf-4367-f0c9-13280fbe1b09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        }
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_dim = 100 # 8\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000, embedding_dim, input_length=maxlen)) # max_features = 10000 sentences, 8 ? # maxlen = 20 max word\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 100)          1000000   \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 10000)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 10001     \n",
            "=================================================================\n",
            "Total params: 1,010,001\n",
            "Trainable params: 1,010,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p51Ol_MDJG6n",
        "colab_type": "code",
        "outputId": "af572237-ee5b-465f-f9a6-8cbfc0ea02ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "6750/6750 [==============================] - 57s 8ms/step - loss: 0.3045 - acc: 0.8732 - val_loss: 0.2792 - val_acc: 0.8895\n",
            "Epoch 2/10\n",
            "6750/6750 [==============================] - 57s 8ms/step - loss: 0.2412 - acc: 0.9060 - val_loss: 0.2973 - val_acc: 0.8852\n",
            "Epoch 3/10\n",
            "6750/6750 [==============================] - 56s 8ms/step - loss: 0.1810 - acc: 0.9327 - val_loss: 0.3349 - val_acc: 0.8768\n",
            "Epoch 4/10\n",
            "6750/6750 [==============================] - 56s 8ms/step - loss: 0.1264 - acc: 0.9553 - val_loss: 0.3889 - val_acc: 0.8634\n",
            "Epoch 5/10\n",
            "6750/6750 [==============================] - 57s 8ms/step - loss: 0.0876 - acc: 0.9702 - val_loss: 0.4570 - val_acc: 0.8547\n",
            "Epoch 6/10\n",
            "6750/6750 [==============================] - 56s 8ms/step - loss: 0.0619 - acc: 0.9797 - val_loss: 0.5277 - val_acc: 0.8469\n",
            "Epoch 7/10\n",
            "6750/6750 [==============================] - 55s 8ms/step - loss: 0.0450 - acc: 0.9854 - val_loss: 0.6106 - val_acc: 0.8406\n",
            "Epoch 8/10\n",
            "6750/6750 [==============================] - 56s 8ms/step - loss: 0.0340 - acc: 0.9895 - val_loss: 0.6923 - val_acc: 0.8315\n",
            "Epoch 9/10\n",
            "6750/6750 [==============================] - 56s 8ms/step - loss: 0.0260 - acc: 0.9921 - val_loss: 0.7832 - val_acc: 0.8286\n",
            "Epoch 10/10\n",
            "6750/6750 [==============================] - 55s 8ms/step - loss: 0.0207 - acc: 0.9937 - val_loss: 0.8644 - val_acc: 0.8258\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLg2MNpruHby",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "62b4ebdf-9835-496e-a99c-72679df7b310"
      },
      "source": [
        "evaluation = model.evaluate(x_test,  y_test,batch_size=32, verbose=2)\n",
        "print()\n",
        "print(\"Test loss :\",evaluation[0]*100,\"%\")\n",
        "print(\"Test accuracy :\",evaluation[1]*100,\"%\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2813/2813 - 8s - loss: 0.8802 - acc: 0.8232\n",
            "\n",
            "Test loss : 88.02236318588257 %\n",
            "Test accuracy : 82.3199987411499 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL2AmwwSpFkY",
        "colab_type": "text"
      },
      "source": [
        "6) From 6.8 code pre-trained word-embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvkewQjmrOIl",
        "colab_type": "text"
      },
      "source": [
        "DOWNLOAD THE GLOVE WORD EMBEDDINGS\n",
        "\n",
        "Its a pretrained Word Embedding : https://nlp.stanford.edu/projects/glove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBYl-DPKFJOZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "47733177-dffd-4f73-e00f-8de1be80944e"
      },
      "source": [
        "# after downloading\n",
        "# im upoading it manulally on my Google drive\n",
        "# and now checking files\n",
        "import os\n",
        "os.listdir(\"/content/drive/My Drive/ML and AI/pretrained word embeddings/glove.6B\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['glove.6B.100d.txt',\n",
              " 'glove.6B.200d.txt',\n",
              " 'glove.6B.50d.txt',\n",
              " 'glove.6B.300d.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE7FUUdIssPw",
        "colab_type": "text"
      },
      "source": [
        "# Parsing the GloVe word-embeddings file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDav_apBsufT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "1fbf859f-8b39-43fe-e435-b23986ff0cdb"
      },
      "source": [
        "# lets check the downloaded glove file\n",
        "\n",
        "import os\n",
        "print(\"Folder contain :\",os.listdir(\"/content/drive/My Drive/ML and AI/pretrained word embeddings/glove.6B\"))\n",
        "print()\n",
        "\n",
        "# each file colntain word with its vextor lets check\n",
        "\n",
        "f = open('/content/drive/My Drive/ML and AI/pretrained word embeddings/glove.6B/glove.6B.100d.txt',encoding=\"utf8\")\n",
        "for i in f:\n",
        "    values = i.split()\n",
        "f.close()\n",
        "print(\"Printing first value :\\n\\n\",values)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Folder contain : ['glove.6B.100d.txt', 'glove.6B.200d.txt', 'glove.6B.50d.txt', 'glove.6B.300d.txt']\n",
            "\n",
            "Printing first value :\n",
            "\n",
            " ['sandberger', '0.28365', '-0.6263', '-0.44351', '0.2177', '-0.087421', '-0.17062', '0.29266', '-0.024899', '0.26414', '-0.17023', '0.25817', '0.097484', '-0.33103', '-0.43859', '0.0095799', '0.095624', '-0.17777', '0.38886', '0.27151', '0.14742', '-0.43973', '-0.26588', '-0.024271', '0.27186', '-0.36761', '-0.24827', '-0.20815', '0.22128', '-0.044409', '0.021373', '0.24594', '0.26143', '0.29303', '0.13281', '0.082232', '-0.12869', '0.1622', '-0.22567', '-0.060348', '0.28703', '0.11381', '0.34839', '0.3419', '0.36996', '-0.13592', '0.0062694', '0.080317', '0.0036251', '0.43093', '0.01882', '0.31008', '0.16722', '0.074112', '-0.37745', '0.47363', '0.41284', '0.24471', '0.075965', '-0.51725', '-0.49481', '0.526', '-0.074645', '0.41434', '-0.1956', '-0.16544', '-0.045649', '-0.40153', '-0.13136', '-0.4672', '0.18825', '0.2612', '0.16854', '0.22615', '0.62992', '-0.1288', '0.055841', '0.01928', '0.024572', '0.46875', '0.2582', '-0.31672', '0.048591', '0.3277', '-0.50141', '0.30855', '0.11997', '-0.25768', '-0.039867', '-0.059672', '0.5525', '0.13885', '-0.22862', '0.071792', '-0.43208', '0.5398', '-0.085806', '0.032651', '0.43678', '-0.82607', '-0.15701']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tm7Kk7a5swe9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a249cf54-458d-44c6-b693-88c6bb9ebcc8"
      },
      "source": [
        "# lets put the same format in a dictionary\n",
        "\n",
        "glove_dir = '/content/drive/My Drive/ML and AI/pretrained word embeddings/glove.6B'                                                   \n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'),encoding=\"utf8\")\n",
        "\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "print()\n",
        "#print(embeddings_index)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgipi3pOs79V",
        "colab_type": "text"
      },
      "source": [
        "# Preparing the GloVe word-embeddings matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niqHkVt7tAdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we have\n",
        "# print(word_index) # word and its number (built during tokenization) in dict format\n",
        "\n",
        "# and we have\n",
        "# print(embeddings_index) # word and its vector (Gloves) in dict format"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTDOBOpAswlh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ea3b1b9b-8cc9-414e-a5c0-0af907005acd"
      },
      "source": [
        "# ab hum embeding matrix bnainge jise embeding layer mai dalsake\n",
        "# word tokinizer wala ho aue uska vector glove wala\n",
        "\n",
        "# zero ke matrix bnaya\n",
        "\n",
        "# max_words = 10000\n",
        "embedding_dim = 100\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "\n",
        "print(embedding_matrix.shape)\n",
        "print()\n",
        "#print(embedding_matrix)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 100)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5Lc5rMNtEht",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "13aa8992-d067-43f2-d2c6-823ca3df58f7"
      },
      "source": [
        "for word, i in word_index.items():\n",
        "    if i < max_words:   # 10000 index uthai tokinizer ke\n",
        "        embedding_vector = embeddings_index.get(word) # aur gloves ka vector usay dedo\n",
        "        if embedding_vector is not None: # aur word na mile embading mai to zero ka bnado\n",
        "            embedding_matrix[i] = embedding_vector # embedding_matrix mai save kerdo\n",
        "\n",
        "# ab hamare pass tokinizer wale value ke vector bhe hai embedding_matrix mai\n",
        "print(embedding_matrix.shape)\n",
        "print()\n",
        "#print(embedding_matrix)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 100)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruu7qPaNtIUu",
        "colab_type": "text"
      },
      "source": [
        "# Model definition\n",
        "Well use the same model architecture as before. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aus-2vrdswqv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "outputId": "7a496c0e-5854-4fc9-cf7e-500c9adb87ad"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "#model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 100, 100)          1000000   \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 10000)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 10001     \n",
            "=================================================================\n",
            "Total params: 1,010,001\n",
            "Trainable params: 1,010,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIaMpVZWtOPW",
        "colab_type": "text"
      },
      "source": [
        "# Loading pretrained word embeddings into the Embedding layer\n",
        "The Embedding layer has a single weight matrix: a 2D float matrix where each entry i is\n",
        "the word vector meant to be associated with index i. Simple enough. Load the GloVe\n",
        "matrix you prepared into the Embedding layer, the first layer in the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ASjN8N9swti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.layers[0].set_weights([embedding_matrix]) # pretrained use kare\n",
        "model.layers[0].trainable = False               # aur usko freeze kerdia ke mazed train na ho"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMvXaMoAtSqh",
        "colab_type": "text"
      },
      "source": [
        "Additionally, youll freeze the Embedding layer (set its trainable attribute to False),\n",
        "following the same rationale youre already familiar with in the context of pretrained\n",
        "convnet features: when parts of a model are pretrained (like your Embedding layer)\n",
        "and parts are randomly initialized (like your classifier), the pretrained parts shouldnt\n",
        "be updated during training, to avoid forgetting what they already know. The large gradient updates triggered by the randomly initialized layers would be disruptive to the\n",
        "already-learned features. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trUKnzjatV-O",
        "colab_type": "text"
      },
      "source": [
        "# Training and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d21dIQvBswov",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "bd55c2ef-ce98-460f-db66-f4dfc394bd3b"
      },
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "loss='binary_crossentropy',\n",
        "metrics=['acc'])\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)\n",
        "\n",
        "#model.save_weights('pre_trained_glove_model.h5')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "6750/6750 [==============================] - 27s 4ms/step - loss: 0.5625 - acc: 0.7382 - val_loss: 0.5115 - val_acc: 0.7640\n",
            "Epoch 2/10\n",
            "6750/6750 [==============================] - 26s 4ms/step - loss: 0.5337 - acc: 0.7643 - val_loss: 0.5131 - val_acc: 0.7746\n",
            "Epoch 3/10\n",
            "6750/6750 [==============================] - 27s 4ms/step - loss: 0.5300 - acc: 0.7676 - val_loss: 0.5220 - val_acc: 0.7704\n",
            "Epoch 4/10\n",
            "6750/6750 [==============================] - 26s 4ms/step - loss: 0.5294 - acc: 0.7697 - val_loss: 0.5489 - val_acc: 0.7578\n",
            "Epoch 5/10\n",
            "6750/6750 [==============================] - 27s 4ms/step - loss: 0.5275 - acc: 0.7700 - val_loss: 0.5971 - val_acc: 0.7428\n",
            "Epoch 6/10\n",
            "6750/6750 [==============================] - 26s 4ms/step - loss: 0.5288 - acc: 0.7703 - val_loss: 0.5812 - val_acc: 0.7593\n",
            "Epoch 7/10\n",
            "6750/6750 [==============================] - 28s 4ms/step - loss: 0.5282 - acc: 0.7704 - val_loss: 0.5937 - val_acc: 0.7575\n",
            "Epoch 8/10\n",
            "6750/6750 [==============================] - 27s 4ms/step - loss: 0.5278 - acc: 0.7711 - val_loss: 0.5294 - val_acc: 0.7692\n",
            "Epoch 9/10\n",
            "6750/6750 [==============================] - 27s 4ms/step - loss: 0.5274 - acc: 0.7720 - val_loss: 0.5534 - val_acc: 0.7596\n",
            "Epoch 10/10\n",
            "6750/6750 [==============================] - 27s 4ms/step - loss: 0.5284 - acc: 0.7717 - val_loss: 0.5720 - val_acc: 0.7645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMnUte9OujLo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "67bd814f-39cb-4dba-ecf7-2ae93568e92e"
      },
      "source": [
        "evaluation = model.evaluate(x_test,y_test,verbose=2)\n",
        "print()\n",
        "print(\"Loss: \",evaluation[0]*100,\"%\")\n",
        "print(\"Accuracy: \",evaluation[1]*100,\"%\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2813/2813 - 8s - loss: 0.5729 - acc: 0.7614\n",
            "\n",
            "Loss:  57.291942834854126 %\n",
            "Accuracy:  76.14444494247437 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmnHKqd6vv95",
        "colab_type": "text"
      },
      "source": [
        "seems like acc was close in both but loss was a lot less is pre-trained word embedding, so we are going to use glove for RNN too."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmBrzvyIoY6u",
        "colab_type": "text"
      },
      "source": [
        "7) Apply RNN to the given text (listing 6.21) and provide output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps33L4zFtYaO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "4e1f4155-5882-43f8-d454-bb85b16443ba"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(SimpleRNN(32, return_sequences=True))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 100, 100)          1000000   \n",
            "_________________________________________________________________\n",
            "simple_rnn_3 (SimpleRNN)     (None, 100, 32)           4256      \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 3200)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 3201      \n",
            "=================================================================\n",
            "Total params: 1,007,457\n",
            "Trainable params: 1,007,457\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfBzpkzcw6Nm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pretrained Glove word embedding\n",
        "model.layers[0].set_weights([embedding_matrix]) # pretrained use kare\n",
        "model.layers[0].trainable = False               # aur usko freeze kerdia ke mazed train na ho"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NexCkt9xwarn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "loss='binary_crossentropy',\n",
        "metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTgTit7AwYVQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "bdc9fa44-e9db-40f3-d929-c8beecc2e671"
      },
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "6750/6750 [==============================] - 435s 64ms/step - loss: 0.4696 - acc: 0.7804 - val_loss: 0.3973 - val_acc: 0.8258\n",
            "Epoch 2/10\n",
            "6750/6750 [==============================] - 436s 65ms/step - loss: 0.3841 - acc: 0.8320 - val_loss: 0.3774 - val_acc: 0.8347\n",
            "Epoch 3/10\n",
            "6750/6750 [==============================] - 432s 64ms/step - loss: 0.3602 - acc: 0.8449 - val_loss: 0.3510 - val_acc: 0.8543\n",
            "Epoch 4/10\n",
            "6750/6750 [==============================] - 431s 64ms/step - loss: 0.3441 - acc: 0.8531 - val_loss: 0.3309 - val_acc: 0.8603\n",
            "Epoch 5/10\n",
            "6750/6750 [==============================] - 432s 64ms/step - loss: 0.3343 - acc: 0.8584 - val_loss: 0.3551 - val_acc: 0.8465\n",
            "Epoch 6/10\n",
            "6750/6750 [==============================] - 429s 64ms/step - loss: 0.3262 - acc: 0.8613 - val_loss: 0.3485 - val_acc: 0.8556\n",
            "Epoch 7/10\n",
            "6750/6750 [==============================] - 432s 64ms/step - loss: 0.3210 - acc: 0.8647 - val_loss: 0.3281 - val_acc: 0.8615\n",
            "Epoch 8/10\n",
            "6750/6750 [==============================] - 436s 65ms/step - loss: 0.3150 - acc: 0.8679 - val_loss: 0.3322 - val_acc: 0.8595\n",
            "Epoch 9/10\n",
            "6750/6750 [==============================] - 440s 65ms/step - loss: 0.3108 - acc: 0.8699 - val_loss: 0.3253 - val_acc: 0.8619\n",
            "Epoch 10/10\n",
            "6750/6750 [==============================] - 423s 63ms/step - loss: 0.3095 - acc: 0.8708 - val_loss: 0.3098 - val_acc: 0.8720\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JB8bGjoMx8U_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "6f17bdef-e755-44ba-8bdd-fbc60cd91db1"
      },
      "source": [
        "evaluation = model.evaluate(x_test,y_test,verbose=2)\n",
        "print()\n",
        "print(\"Loss: \",evaluation[0]*100,\"%\")\n",
        "print(\"Accuracy: \",evaluation[1]*100,\"%\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2813/2813 - 29s - loss: 0.3132 - acc: 0.8724\n",
            "\n",
            "Loss:  31.32464289665222 %\n",
            "Accuracy:  87.24222183227539 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoFFAVSPyGuW",
        "colab_type": "text"
      },
      "source": [
        "8) Match the results of RNN (step 7) with the step 5 and also with step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJSgSAupxvt5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# embedding layer usually work better than one hot encoding\n",
        "# pre-trained word embeding work better than our own embadding layer here\n",
        "# rnn takes a lot of time to process but out performe every thing.\n",
        "# We didn't use LSTM and GRU because not included in assighnment,\n",
        "# But both of them perform close to like RNN and take less time (computational power)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyUX7Sy-yYLA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note we limited the data above and use less data for this entire assighnment\n",
        "# we can undo that and check the performance than, which will be better than now.\n",
        "# feel free to try, im not going to try it now because its going to take alot of time."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_vhcweHzCab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}